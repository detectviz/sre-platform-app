## 緊急應變

作者：Corey Adam Baye 編輯：Diane Bates

> 事情會出錯；這就是人生。

無論涉及的風險有多大，或組織的規模有多大，對組織的長期健康至關重要，並因此使該組織與眾不同的一個特質是，相關人員如何應對緊急情況。我們中很少有人在緊急情況下能自然地應對得很好。一個適當的回應需要準備和定期的、相關的、實踐的培訓。建立和維持全面的培訓和測試流程需要董事會和管理層的支持，以及員工的細心關注。所有這些因素對於培養一個環境至關重要，在這個環境中，團隊可以花費金錢、時間、精力，甚至可能是正常運行時間，以確保系統、流程和人員在緊急情況下能夠高效地應對。

請注意，關於事後檢討文化的章節討論了如何撰寫事後檢討的具體細節，以確保需要緊急應變的事件也能成為一個學習的機會（請參閱「事後檢討文化：從失敗中學習」）。本章提供了此類事件的更具體的例子。

# 當系統崩潰時該怎麼辦

首先，不要驚慌！你不是一個人，天也沒有塌下來。你是一名專業人士，受過處理這種情況的訓練。通常，沒有人處於身體危險之中——只有那些可憐的電子處於危險之中。在最壞的情況下，一半的網際網路都癱瘓了。所以深呼吸……然後繼續。

如果你感到不知所措，就多找些人來。有時甚至可能需要呼叫整個公司。如果你的公司有事件應變流程（請參閱「管理事件」），請確保你熟悉它並遵循該流程。

# 測試引發的緊急情況

Google 已經採用了一種積極主動的方法來進行災難和緊急情況測試（請參閱 [Kri12]）。SRE 會破壞我們的系統，觀察它們如何失敗，並進行更改以提高可靠性並防止故障再次發生。大多數時候，這些受控的失敗都按計劃進行，目標系統和依賴系統的行為大致符合我們的預期。我們識別出一些弱點或隱藏的依賴關係，並記錄後續行動以糾正我們發現的缺陷。然而，有時我們的假設與實際結果相去甚遠。

這裡有一個測試的例子，它揭示了許多意想不到的依賴關係。

## 細節

我們想要在我們一個較大的分散式 MySQL 資料庫中，找出對一個測試資料庫的隱藏依賴。計劃是阻止對一百個資料庫中的僅僅一個資料庫的所有存取。沒有人預見到將會發生的結果。

## 回應

測試開始後幾分鐘內，許多依賴的服務報告說，外部和內部用戶都無法存取關鍵系統。一些系統間歇性或僅部分可存取。

假設測試是罪魁禍首，SRE 立即中止了該演習。我們試圖回滾權限變更，但未成功。我們沒有驚慌，而是立即集思廣益，思考如何恢復正常的存取。使用一個已經過測試的方法，我們恢復了副本和故障轉移的權限。在一個並行的努力中，我們聯繫了關鍵的開發人員，以糾正資料庫應用程式層函式庫中的缺陷。

在最初決定後的一小時內，所有存取都完全恢復了，所有服務都能再次連接。這次測試的廣泛影響促使我們對函式庫進行了快速而徹底的修復，並制定了定期重新測試的計劃，以防止類似的重大缺陷再次發生。

## 發現

### 做得好的地方

- 受事件影響的依賴服務立即在公司內部升級了問題。
- 我們正確地假設，我們的受控實驗已經失控，並立即中止了測試。
- 我們在第一份報告後的一小時內完全恢復了權限，此時系統開始正常運行。一些團隊採取了不同的方法，重新配置他們的系統以避開測試資料庫。這些並行的努力有助於盡快恢復服務。
- 後續的行動項目被迅速而徹底地解決，以避免類似的中斷，並且我們建立了定期測試以確保類似的缺陷不再發生。

### 我們學到的

- 儘管這次測試經過了徹底的審查，並被認為範圍界定得很好，但現實揭示了我們對依賴系統之間這種特定互動的理解不足。
- 我們未能遵循事件應變流程，該流程僅在幾週前制定，並且尚未被徹底傳播。這個流程本可以確保所有服務和客戶都意識到這次中斷。為了避免將來出現類似情況，SRE 不斷完善和測試我們的事件應變工具和流程，此外還確保我們的事件管理程序的更新被清楚地傳達給所有相關方。
- 因為我們沒有在測試環境中測試我們的回滾程序，這些程序存在缺陷，這延長了中斷時間。我們現在要求在進行此類大規模測試之前，對回滾程序進行徹底的測試。

# 變更引發的緊急情況

正如您可以想像的，Google 有大量的配置——複雜的配置——而且我們不斷地對這些配置進行變更。為了防止我們的系統徹底崩潰，我們對配置變更進行了大量的測試，以確保它們不會導致意想不到和不期望的行為。然而，Google 基礎設施的規模和複雜性使得不可能預測到每一個依賴關係或互動；有時配置變更並不完全按計劃進行。

以下就是這樣一個例子。

## 細節

一個週五，一個對有助於保護我們服務免受濫用的基礎設施的配置變更被全球推送。這個基礎設施基本上與我們所有對外的系統互動，而這個變更觸發了那些系統中的一個崩潰循環錯誤，導致整個機群幾乎同時開始崩潰循環。因為 Google 的內部基礎設施也依賴於我們自己的服務，許多內部應用程式也突然變得不可用。

## 回應

幾秒鐘內，監控警報開始觸發，表明某些站點已宕機。一些值班工程師同時經歷了他們認為是公司網路的故障，並搬遷到專用的安全室（緊急應變室），那裡有備份的生產環境存取權限。其他正在與他們的企業存取權限作鬥爭的工程師也加入了他們。

在第一次配置推送後的五分鐘內，負責推送的工程師意識到公司中斷，但仍未意識到更廣泛的中斷，他推送了另一個配置變更以回滾第一個變更。此時，服務開始恢復。

在第一次推送後的 10 分鐘內，值班工程師宣布發生事件，並開始遵循內部事件應變程序。他們開始通知公司其他部門有關情況。推送工程師告知值班工程師，中斷很可能是由於已被推送並後來回滾的變更引起的。儘管如此，一些服務經歷了由原始事件觸發的不相關的錯誤或配置錯誤，並且長達一小時才完全恢復。

## 發現

### 做得好的地方

- 有幾個因素在起作用，阻止了這次事件導致 Google 許多內部系統的長期中斷。
- 首先，監控幾乎立即檢測到並向我們發出了問題警報。然而，應該指出的是，在這種情況下，我們的監控並不理想：警報反覆不斷地觸發，淹沒了值班人員，並向常規和緊急通信渠道發送了垃圾郵件。
- 一旦問題被檢測到，事件管理總體上進展順利，更新被頻繁且清晰地傳達。我們的**帶外 (out-of-band)** 通信系統即使在一些更複雜的軟體堆疊無法使用時，也讓每個人都保持聯繫。這次經歷提醒我們為什麼 SRE 保留高度可靠、低開銷的備份系統，我們經常使用這些系統。
- 除了這些帶外通信系統，Google 還有命令列工具和替代存取方法，使我們能夠在其他介面無法存取時執行更新和回滾變更。這些工具和存取方法在停機期間運作良好，但需要注意的是，工程師需要更熟悉這些工具，並更常規地進行測試。
- Google 的基礎設施提供了另一層保護，即受影響的系統對向新客戶端提供完整更新的速度進行了**速率限制 (rate-limited)**。這種行為可能抑制了崩潰循環，並防止了完全中斷，允許工作在崩潰之間保持足夠長的時間來服務一些請求。
- 最後，我們不應忽視這次事件快速解決中的運氣因素：推送工程師碰巧在關注即時通信渠道——這是一種額外的勤奮，並非發布流程的常規部分。推送工程師在推送後直接注意到大量關於企業存取的投訴，並幾乎立即回滾了變更。如果沒有這次迅速的回滾，中斷可能會持續更長時間，變得極其難以排除故障。

### 我們學到的

- 一個早期的新功能推送涉及了徹底的**金絲雀測試 (canary)**，但沒有觸發相同的錯誤，因為它沒有在與新功能結合的情況下執行一個非常罕見和特定的配置關鍵字。觸發這個錯誤的特定變更不被認為是高風險的，因此遵循了一個不那麼嚴格的金絲雀測試流程。當該變更被全球推送時，它使用了未經測試的關鍵字/功能組合，從而觸發了故障。
- 諷刺的是，對金絲雀測試和自動化的改進被安排在下個季度成為更高的優先級。這次事件立即提高了它們的優先級，並加強了無論風險被認為有多大，都需要進行徹底的金絲雀測試的必要性。
- 正如人們所預料的，這次事件期間警報非常活躍，因為每個地點基本上都離線了幾分鐘。這擾亂了值班工程師正在進行的實際工作，並使事件相關人員之間的溝通更加困難。
- Google 依賴於我們自己的工具。我們用於故障排除和溝通的大部分軟體堆疊都位於正在崩潰循環的工作之後。如果這次中斷持續更長時間，除錯將會受到嚴重阻礙。

# 流程引發的緊急情況

我們在管理我們機器機隊的自動化上投入了大量的時間和精力。只需很少的努力，就可以在整個機隊中啟動、停止或改造大量的作業，這真是令人驚訝。有時，當事情不完全按計劃進行時，我們自動化的效率可能會有點嚇人。

這是一個快速行動並非好事的一個例子。

## 細節

作為例行自動化測試的一部分，提交了兩個連續的針對同一個即將退役的伺服器安裝的停用請求。在第二個停用請求的情況下，自動化中的一個細微錯誤將所有這些全球安裝中的所有機器都發送到了 Diskerase 佇列，在那裡它們的硬碟注定要被擦除；更多細節請參閱「自動化：大規模啟用故障」。

## 回應

在第二個停用請求發出後不久，值班工程師收到了一個呼叫，因為第一個小型伺服器安裝被離線以進行退役。他們的調查確定這些機器已被轉移到 Diskerase 佇列，因此按照正常程序，值班工程師從該地點排空了流量。由於該地點的機器已被擦除，它們無法回應請求。為了避免直接讓這些請求失敗，值班工程師將流量從該地點排走。流量被重定向到能夠正確回應請求的地點。

不久之後，世界各地所有此類伺服器安裝的呼叫器都響了起來。作為回應，值班工程師禁用了所有團隊的自動化，以防止進一步的損壞。他們不久之後停止或凍結了額外的自動化和生產維護。

一小時內，所有流量都已轉移到其他地點。儘管用戶可能會遇到延遲升高，但他們的請求得到了滿足。中斷正式結束。

現在困難的部分開始了：恢復。一些網路連結報告嚴重擁堵，因此網路工程師在瓶頸出現時實施了緩解措施。選擇了其中一個地點的一個伺服器安裝作為眾多從灰燼中重生的第一個。在最初中斷後的三小時內，由於幾位工程師的堅韌不拔，該安裝被重建並重新上線，再次愉快地接受用戶請求。

美國團隊將工作交接給他們的歐洲同事，SRE 制定了一個計劃，使用一個精簡但手動的流程來優先進行重新安裝。團隊被分成三部分，每部分負責手動重新安裝過程中的一個步驟。三天內，絕大多數容量都已恢復上線，而任何落後者將在接下來的一兩個月內恢復。

## 發現

### 做得好的地方

- 大型伺服器安裝中的反向代理與這些小型安裝中的反向代理管理方式非常不同，因此大型安裝未受影響。值班工程師能夠迅速將流量從小型安裝轉移到大型安裝。根據設計，這些大型安裝可以輕鬆處理全部負載。然而，一些網路連結變得擁堵，因此需要網路工程師開發變通辦法。為了減少對終端用戶的影響，值班工程師將擁堵的網路作為他們的最高優先級。
- 小型安裝的停用過程高效且運作良好。從開始到結束，成功停用並安全擦除大量這些安裝只花了一個小時不到。
- 儘管停用自動化迅速拆除了小型安裝的監控，但值班工程師能夠及時恢復那些監控變更。這樣做幫助他們評估了損壞的程度。
- 工程師們迅速遵循了事件應變協定，該協定在本次事件發生前的一年裡已經成熟了很多。整個公司和跨團隊的溝通與協作非常出色——這是對事件管理計畫和培訓的真正證明。各個團隊的所有人都伸出援手，貢獻了他們豐富的經驗。

### 我們學到的

- 根本原因是停用自動化伺服器對其發送的命令缺乏適當的健全性檢查。當伺服器為回應最初失敗的停用而再次運行時，它收到了一個關於機器機架的空回應。它沒有過濾該回應，而是將空過濾器傳遞給了機器資料庫，告訴機器資料庫 Diskerase 所有相關的機器。是的，有時零確實意味著全部。機器資料庫照辦了，所以停用工作流程開始盡可能快地處理機器。
- 機器的重新安裝緩慢且不可靠。這種行為很大程度上是由於從遠端地點使用最低網路服務品質 (QoS) 的**小型檔案傳輸協定 (Trivial File Transfer Protocol, TFTP)** 造成的。系統中每台機器的 BIOS 對故障的處理很差。77 根據所涉及的網卡，BIOS 要麼停止，要麼進入持續的重啟循環。它們在每個循環中都無法傳輸引導檔案，並且進一步給安裝程式帶來了負擔。值班工程師通過將安裝流量重新分類為稍高的優先級，並使用自動化來重啟任何卡住的機器，從而解決了這些重新安裝問題。
- 機器重新安裝基礎設施無法處理數千台機器的同時設定。這種無能部分是由於一個回歸錯誤，該錯誤阻止了基礎設施在每個工作機器上運行超過兩個設定任務。該回歸還使用了不當的 QoS 設定來傳輸檔案，並且超時時間調整得很差。它強制進行核心重新安裝，即使在那些仍然擁有正確核心且 Diskerase 尚未發生的機器上也是如此。為了糾正這種情況，值班工程師向負責該基礎設施的各方進行了升級，他們能夠迅速地重新調整它以支持這種不尋常的負載。

# 所有問題都有解決方案

時間和經驗表明，系統不僅會出錯，而且會以人們以前無法想像的方式出錯。Google 學到的最重要的一課是，解決方案是存在的，即使它可能不明顯，尤其是對那個呼叫器正在尖叫的人來說。如果你想不出解決方案，就把網撒得更遠一些。讓更多的隊友參與進來，尋求幫助，做你必須做的一切，但要快。最高優先級是迅速解決手頭的問題。通常，擁有最多狀態的人是那個其行為以某種方式觸發了事件的人。利用那個人。

非常重要的是，一旦緊急情況得到緩解，不要忘記留出時間來清理、撰寫事件報告，以及……

# 從過去中學習。不要重蹈覆轍。

## 保留中斷的歷史記錄

沒有比記錄過去發生的錯誤更好的學習方法了。歷史就是從每個人的錯誤中學習。要徹底，要誠實，但最重要的是，要提出尖銳的問題。尋找可能防止此類中斷再次發生的具體行動，不僅僅是戰術上的，也是戰略上的。確保公司內的每個人都能通過發布和組織**事後檢討 (postmortems)** 來學習你所學到的東西。

讓自己和他人對遵循這些事後檢討中詳述的具體行動負責。這樣做將防止未來發生與已經記錄在案的中斷幾乎相同，且由幾乎相同的觸發因素引起的中斷。一旦你在從過去的中斷中學習方面有了可靠的記錄，看看你能做些什麼來預防未來的中斷。

## 問那些大的，甚至是不太可能的問題：如果…？

沒有比現實更大的考驗了。問自己一些大的、開放式的問題。如果大樓停電了怎麼辦？如果網路設備機架泡在兩英尺深的水里怎麼辦？如果主資料中心突然停電了怎麼辦？如果有人入侵了你的網頁伺服器怎麼辦？你該怎麼辦？你該打電話給誰？誰來買單？你有計劃嗎？你知道如何反應嗎？你知道你的系統會如何反應嗎？如果現在發生，你能將影響降到最低嗎？你旁邊的人能做到同樣的事情嗎？

## 鼓勵主動測試

在失敗方面，理論和現實是兩個截然不同的領域。在你的系統真正失敗之前，你並不真正知道該系統、其依賴系統或你的用戶將如何反應。不要依賴假設或你不能或沒有測試過的東西。你更願意讓一個失敗發生在周六凌晨 2 點，當公司大部分人還在黑森林進行團隊建設活動時，還是在你有最優秀的人才在身邊，監控著他們在過去幾週裡 painstakingly review 的測試時？

# 結論

我們回顧了三個我們的系統部分出現故障的不同案例。儘管這三個緊急情況的觸發方式不同——一個是由主動測試引發的，另一個是由配置變更引發的，還有一個是由停用自動化引發的——但其應對措施有許多共同之處。應對者沒有驚慌。他們在認為必要時引入了其他人。應對者研究並從早期的中斷中學習。隨後，他們建立了他們的系統以更好地應對那些類型的中- **測試引發的緊急情況**。每次新的故障模式出現時，應對者都會記錄下這些故障模式。這種後續行動幫助其他團隊學習如何更好地進行故障排除，並加固他們的系統以應對類似的中斷。應對者主動測試他們的系統。這種測試確保了變更修復了根本問題，並在它們成為中斷之前識別出其他弱點。

隨著我們的系統不斷演進，這個循環也在繼續，每一次中斷或測試都會帶來流程和系統的增量改進。雖然本章中的案例研究是針對 Google 的，但這種應急應變方法可以隨著時間的推移應用於任何規模的任何組織。

77 BIOS：基本輸入/輸出系統。BIOS 是內建於計算機中的軟體，用於向硬體發送簡單的指令，允許在作業系統加載之前進行輸入和輸出。