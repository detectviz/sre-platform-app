# 管理關鍵狀態：用於可靠性的分散式共識

作者：Laura Nolan
編輯：Tim Harvey

進程會崩潰，或者可能需要重新啟動。硬碟會故障。自然災害可能會摧毀一個地區的幾個資料中心。網站可靠性工程師需要預見這些類型的故障，並制定策略以在這些故障發生時保持系統運行。這些策略通常需要在多個站點運行這樣的系統。地理上分散系統相對直接，但同時也引入了維護系統狀態一致視圖的需求，這是一項更為細膩和困難的任務。

一組進程可能希望可靠地就以下問題達成一致：

-   哪一個進程是一組進程的領導者？
-   一組進程中的進程集合是什麼？
-   一個訊息是否已成功提交到分散式佇列？
-   一個進程是否持有租約？
-   在資料儲存中，給定鍵的值是什麼？

我們發現**分散式共識 (distributed consensus)** 在建立需要對某些系統狀態有一致視圖的可靠且高可用的系統方面非常有效。分散式共識問題處理的是在一組由不可靠通訊網路連接的進程之間達成協議。例如，分散式系統中的幾個進程可能需要能夠對關鍵的配置片段、分散式鎖是否被持有，或者佇列上的訊息是否已被處理，形成一致的視圖。它是分散式計算中最基本的概念之一，我們幾乎為我們提供的每一項服務都依賴它。圖 23-1 說明了一組進程如何通過分散式共識實現對系統狀態的一致視圖的簡單模型。

每當您看到**領導者選舉 (leader election)**、**關鍵共享狀態 (critical shared state)** 或**分散式鎖定 (distributed locking)** 時，我們建議使用經過正式證明和徹底測試的分散式共識系統。解決此問題的非正式方法可能導致服務中斷，更陰險的是，可能導致細微且難以修復的數據一致性問題，從而不必要地延長您系統的中斷時間。

**CAP 定理**（[Fox99]、[Bre12]）認為，一個分散式系統不能同時具備以下所有三個屬性：

-   每個節點上數據的一致視圖
-   每個節點上數據的可用性
-   對網路分區的容忍度 [Gil02]

這個邏輯很直觀：如果兩個節點無法通訊（因為網路被分區），那麼整個系統要麼可以停止在某些或所有節點上服務某些或所有請求（從而降低可用性），要麼它可以像往常一樣服務請求，這會導致每個節點上的數據視圖不一致。

由於網路分區是不可避免的（電纜被切斷、封包因擁塞而丟失或延遲、硬體損壞、網路組件配置錯誤等），理解分散式共識實際上就是理解一致性和可用性如何為您的特定應用程式工作。商業壓力通常要求高水平的可用性，而許多應用程式需要其數據的一致視圖。

系統和軟體工程師通常熟悉傳統的 **ACID** 資料儲存語義（原子性、一致性、隔離性、持久性），但越來越多的分散式資料儲存技術提供了一套不同的語義，稱為 **BASE**（基本可用、軟狀態、最終一致性）。支持 BASE 語義的資料儲存對於某些類型的數據有用的應用，並且可以處理大量的數據和交易，而使用支持 ACID 語義的資料儲存，成本會高得多，甚至可能完全不可行。

大多數支持 BASE 語義的系統都依賴於**多主複製 (multimaster replication)**，其中寫入可以並發地提交到不同的進程，並且有某種機制來解決衝突（通常像「最新時間戳獲勝」一樣簡單）。這種方法通常被稱為**最終一致性 (eventual consistency)**。然而，最終一致性可能導致令人驚訝的結果 [Lu15]，特別是在時鐘漂移（這在分散式系統中是不可避免的）或網路分區 [Kin15] 的情況下。112

對於開發人員來說，設計能夠與僅支持 BASE 語義的資料儲存良好協作的系統也很困難。例如，Jeff Shute [Shu13] 表示，「我們發現開發人員花費了他們大量的時間來建立極其複雜且容易出錯的機制，以應對最終一致性並處理可能過時的數據。我們認為這是對開發人員不可接受的負擔，一致性問題應該在資料庫層面解決。」

系統設計師不能為了實現可靠性或性能而犧牲正確性，特別是在關鍵狀態方面。例如，考慮一個處理金融交易的系統：如果金融數據不正確，可靠性或性能要求就沒有太大價值。系統需要能夠在多個進程之間可靠地同步關鍵狀態。分散式共識演算法提供了這種功能。

## 使用共識的動機：分散式系統協調失敗

分散式系統複雜且難以理解、監控和故障排除。運行此類系統的工程師常常對其在故障情況下的行為感到驚訝。故障是相對罕見的事件，在這些條件下測試系統並不是常規做法。在故障期間很難推斷系統行為。網路分區尤其具有挑戰性——一個看起來是由於完全分區引起的問題，實際上可能是以下原因的結果：

-   一個非常慢的網路
-   一些（但不是全部）訊息被丟棄
-   節流發生在一個方向，但另一個方向沒有

以下各節提供了在現實世界分散式系統中發生的問題示例，並討論了如何使用領導者選舉和分散式共識演算法來預防此類問題。

## 案例研究 1：腦裂問題 (The Split-Brain Problem)

一個服務是一個內容儲存庫，允許多個用戶之間進行協作。它使用位於不同機架中的兩組複製檔案伺服器來確保可靠性。該服務需要避免同時向一組中的兩個檔案伺服器寫入數據，因為這樣做可能導致數據損壞（並可能導致數據無法恢復）。

每對檔案伺服器都有一個領導者和一個追隨者。伺服器通過心跳相互監控。如果一個檔案伺服器無法聯繫其夥伴，它會向其夥伴節點發出 **STONITH (Shoot The Other Node in the Head)** 命令以關閉該節點，然後取得其檔案的主控權。這種做法是減少腦裂實例的行業標準方法，儘管我們將會看到，它在概念上是不健全的。

如果網路變慢或開始丟棄封包會發生什麼？在這種情況下，檔案伺服器會超過其心跳超時，並按照設計向其夥伴節點發送 STONITH 命令並取得主控權。然而，由於網路受損，一些命令可能無法送達。檔案伺服器對現在可能處於這樣一種狀態：兩個節點都預期對同一資源處於活動狀態，或者兩個節點都因為都發出並收到了 STONITH 命令而宕機。這會導致數據損壞或不可用。

這裡的問題是，系統試圖使用簡單的超時來解決領導者選舉問題。領導者選舉是分散式非同步共識問題的重新表述，無法僅使用心跳來正確解決。

## 案例研究 2：故障轉移需要人工干預

一個高度分片的資料庫系統為每個分片都有一個主節點，該主節點同步複製到另一個資料中心的輔助節點。一個外部系統檢查主節點的健康狀況，如果它們不再健康，則將輔助節點提升為主節點。如果主節點無法確定其輔助節點的健康狀況，它會使自己不可用，並將問題升級給人類，以避免案例研究 1 中看到的腦裂情境。

這個解決方案不會有數據丟失的風險，但它確實對數據的可用性產生了負面影響。它還不必要地增加了運行該系統的工程師的操作負載，而且人工干預的擴展性很差。這種主節點和輔助節點通訊出現問題的事件，很可能在發生較大的基礎設施問題時發生，此時響應的工程師可能已經被其他任務壓得喘不過氣來。如果網路受到嚴重影響，以至於分散式共識系統無法選舉出一個主節點，那麼人類很可能也無法更好地做到這一點。

## 案例研究 3：錯誤的組成員演算法

一個系統有一個執行索引和搜尋服務的組件。啟動時，節點使用**八卦協議 (gossip protocol)** 來發現彼此並加入叢集。叢集選舉一個領導者，該領導者執行協調工作。在網路分區將叢集一分為二的情況下，每一邊都（錯誤地）選舉一個主節點並接受寫入和刪除，導致腦裂情境和數據損壞。

在一組進程中確定組成員的一致視圖的問題，是分散式共識問題的另一個實例。

事實上，許多分散式系統問題最終都變成了分散式共識的不同版本，包括主節點選舉、組成員、各種分散式鎖定和租約、可靠的分散式佇列和訊息傳遞，以及任何必須在一組進程中被一致查看的關鍵共享狀態的維護。所有這些問題都應該只使用經過正式證明其正確性，並且其實現經過廣泛測試的分散式共識演算法來解決。解決這類問題的臨時方法（如心跳和八卦協議）在實踐中總是會有可靠性問題。

# 分散式共識如何運作

共識問題有多種變體。在處理分散式軟體系統時，我們感興趣的是**非同步分散式共識 (asynchronous distributed consensus)**，它適用於訊息傳遞延遲可能無限的環境。（**同步共識 (Synchronous consensus)** 適用於即時系統，其中專用硬體意味著訊息將始終在特定的時序保證下傳遞。）

分散式共識演算法可能是**崩潰-失敗 (crash-fail)**（假設崩潰的節點永遠不會返回系統）或**崩潰-恢復 (crash-recover)**。崩潰-恢復演算法更有用，因為真實系統中的大多數問題本質上是暫時性的，例如由於網路緩慢、重啟等。

演算法可能處理**拜占庭 (Byzantine)** 或非拜占庭故障。拜占庭故障發生在一個進程由於錯誤或惡意活動而傳遞不正確的訊息時，處理起來成本相對較高，且較少遇到。

技術上講，在有限時間內解決非同步分散式共識問題是不可能的。正如獲得戴克斯特拉獎 (Dijkstra Prize) 的 **FLP 不可能性結果** [Fis85] 所證明的，在存在不可靠網路的情況下，沒有非同步分散式共識演算法可以保證進展。

在實踐中，我們通過確保系統在大多數時間內擁有足夠的健康副本和網路連接以可靠地取得進展，來在有限時間內處理分散式共識問題。此外，系統應該有帶有隨機延遲的退避機制。這種設置既可以防止重試引起級聯效應，也可以避免本章後面描述的**競爭提議者 (dueling proposers)** 問題。協議保證了安全性，系統中足夠的冗餘則促進了活性。

分散式共識問題的原始解決方案是 Lamport 的 **Paxos** 協議 [Lam98]，但還有其他解決該問題的協議，包括 **Raft** [Ong14]、**Zab** [Jun11] 和 **Mencius** [Mao08]。Paxos 本身有許多旨在提高性能的變體 [Zoo14]。這些通常只在一個細節上有所不同，例如給予一個進程特殊的領導者角色以簡化協議。

## Paxos 概述：一個範例協議

Paxos 作為一系列提案運作，這些提案可能被系統中大多數進程接受，也可能不被接受。如果一個提案未被接受，它就失敗了。每個提案都有一個序列號，這對系統中所有的操作強加了一個嚴格的順序。

在協議的第一階段，提議者向接受者發送一個序列號。每個接受者只有在尚未看到具有更高序列號的提案時，才會同意接受該提案。如果需要，提議者可以用更高的序列號重試。提議者必須使用唯一的序列號（例如，從不相交的集合中抽取，或將其主機名納入序列號中）。

如果一個提-議者從大多數接受者那裡獲得了同意，它就可以通過發送一個帶有值的提交訊息來提交該提案。

提案的嚴格排序解決了系統中與訊息排序相關的任何問題。要求大多數節點提交意味著不能為同一個提案提交兩個不同的值，因為任何兩個多數派至少會在一個節點上重疊。接受者每次同意接受一個提案時，都必須在持久性儲存上寫入日誌，因為接受者在重啟後需要遵守這些保證。

Paxos 本身並不是那麼有用：它只讓您能夠一次性地就一個值和提案號達成一致。因為只需要一個節點的法定人數 (quorum) 同意一個值，所以任何給定的節點可能都沒有已達成一致的值集合的完整視圖。這個限制對大多數分散式共識演算法都是如此。

# 分散式共識的系統架構模式

分散式共識演算法是低階和原始的：它們僅僅允許一組節點一次性地就一個值達成一致。它們不太適合真實的設計任務。使分散式共識有用的是增加了更高級別的系統組件，例如資料儲存、配置儲存、佇列、鎖定和領導者選舉服務，以提供分散式共識演算法未解決的實用系統功能。使用更高級別的組件為系統設計師降低了複雜性。它還允許在必要時更改底層的分散式共識演算法，以應對系統運行環境的變化或非功能性需求的變化。

許多成功使用共識演算法的系統實際上是作為實現這些演算法的某些服務的客戶端，例如 Zookeeper、Consul 和 etcd。**Zookeeper** [Hun10] 是第一個在業界獲得廣泛應用的開源共識系統，因為它易於使用，即使對於那些並非為使用分散式共識而設計的應用程式也是如此。**Chubby** 服務在 Google 填補了類似的利基市場。其作者指出 [Bur06]，將共識原語作為服務提供，而不是作為工程師內建到其應用程式中的函式庫，使應用程式維護者不必以與高可用性共識服務相容的方式部署其系統（運行正確數量的副本、處理組成員、處理性能等）。

## 可靠的複製狀態機 (Reliable Replicated State Machines)

**複製狀態機 (replicated state machine, RSM)** 是一個在多個進程上以相同順序執行相同操作集的系統。RSM 是有用的分散式系統組件和服務（如數據或配置儲存、鎖定和領導者選舉，稍後將詳細描述）的基本構建模塊。

RSM 上的操作是通過共識演算法全域排序的。這是一個強大的概念：幾篇論文（[Agu10]、[Kir08]、[Sch90]）表明，任何確定性程式都可以通過實現為 RSM 來實現為高可用的複製服務。

如圖 23-2 所示，複製狀態機是在共識演算法之上的邏輯層實現的系統。共識演算法處理操作序列的協議，而 RSM 按該順序執行操作。因為並非共識組的每個成員都必然是每個共識法定人數的成員，所以 RSM 可能需要從對等方同步狀態。正如 Kirsch 和 Amir [Kir08] 所描述的，您可以使用滑動窗口協議來協調 RSM 中對等進程之間的狀態。

## 可靠的複製資料儲存和配置儲存

可靠的複製資料儲存是複製狀態機的一個應用。複製資料儲存在其工作的關鍵路徑中使用共識演算法。因此，性能、吞吐量和擴展能力在這種類型的設計中非常重要。與使用其他底層技術構建的資料儲存一樣，基於共識的資料儲存可以為讀取操作提供多種一致性語義，這對資料儲存的擴展方式產生了巨大影響。這些權衡在《分散式共識性能》中進行了討論。

其他（非基於分散式共識的）系統通常僅依賴時間戳來提供返回數據年齡的界限。時間戳在分散式系統中問題重重，因為無法保證多台機器上的時鐘是同步的。**Spanner** [Cor12] 通過建模所涉及的最壞情況不確定性，並在必要時減慢處理速度以解決該不確定性，來解決這個問題。

## 使用領導者選舉的高可用性處理

分散式系統中的領導者選舉是與分散式共識等價的問題。使用單一領導者來執行系統中某些特定類型工作的複製服務非常常見；單一領導者機制是在粗粒度級別上確保互斥的一種方式。

這種類型的設計適用於服務領導者的工作可以由一個進程執行或是分片的情況。系統設計師可以通過將其編寫為好像是一個簡單的程式，複製該進程並使用領導者選舉來確保在任何時間點只有一個領導者在工作，從而構建一個高可用的服務（如圖 23-3 所示）。通常，領導者的工作是協調系統中的某個工作者池。這種模式曾被用於 GFS [Ghe03]（已被 Colossus 取代）和 Bigtable 鍵值儲存 [Cha06]。

在這種類型的組件中，與複製資料儲存不同，共識演算法不在系統正在做的主體工作的關鍵路徑上，因此吞吐量通常不是主要問題。

## 分散式協調與鎖定服務

分散式計算中的**屏障 (barrier)** 是一種原語，它阻止一組進程繼續進行，直到滿足某個條件（例如，直到計算的一個階段的所有部分都完成）。使用屏障有效地將分散式計算分割成邏輯階段。例如，如圖 23-4 所示，在實現 MapReduce [Dea04] 模型時可以使用屏障，以確保在計算的 Reduce 部分繼續之前，整個 Map 階段都已完成。

屏障可以由單個協調器進程實現，但這種實現增加了一個通常不可接受的單點故障。屏障也可以實現為 RSM。Zookeeper 共識服務可以實現屏障模式：見 [Hun10] 和 [Zoo14]。

**鎖 (Locks)** 是另一種有用的協調原語，可以實現為 RSM。考慮一個分散式系統，其中工作進程原子性地消耗一些輸入檔案並寫入結果。分散式鎖可用於防止多個工作者處理同一個輸入檔案。在實踐中，使用帶有超時的可再生**租約 (leases)** 而不是無限期鎖是至關重要的，因為這樣可以防止因進程崩潰而無限期地持有鎖。分散式鎖定超出了本章的範圍，但請記住，分散式鎖是一種低階系統原語，應謹慎使用。大多數應用程式應使用提供分散式交易的更高級別的系統。

## 可靠的分散式佇列與訊息傳遞

佇列是一種常見的數據結構，通常用作在多個工作進程之間分配任務的方式。

基於佇列的系統可以相對容易地容忍工作節點的故障和丟失。然而，系統必須確保聲稱的任務被成功處理。為此，建議使用租約系統（前面在鎖的部分討論過），而不是直接從佇列中移除。基於佇列的系統的缺點是，佇列的丟失會阻止整個系統的運行。將佇列實現為 RSM 可以最小化風險，並使整個系統更加穩健。

**原子廣播 (Atomic broadcast)** 是一種分散式系統原語，其中訊息被所有參與者可靠地以相同的順序接收。這是一個極其強大的分散式系統概念，在設計實用系統時非常有用。系統設計師可以使用大量的發布-訂閱訊息基礎設施，儘管並非所有都提供原子保證。Chandra 和 Toueg [Cha96] 證明了原子廣播和共識的等價性。

將佇列作為工作分配的模式，即使用佇列作為負載平衡設備，如圖 23-5 所示，可以被視為點對點訊息傳遞。訊息系統通常也實現**發布-訂閱 (publish-subscribe)** 佇列，其中訊息可以被訂閱某個頻道或主題的許多客戶端消費。在這種一對多的情況下，佇列上的訊息被儲存為一個持久的有序列表。發布-訂閱系統可用於需要客戶端訂閱以接收某種類型事件通知的許多類型的應用程式。發布-訂閱系統也可用於實現一致的分散式快取。

佇列和訊息系統通常需要出色的吞吐量，但不需要極低的延遲（因為很少直接面向用戶）。然而，在像剛才描述的那樣，有多個工作者從佇列中領取任務的系統中，非常高的延遲可能會成為一個問題，如果每個任務的處理時間百分比顯著增長的話。

# 分散式共識性能

傳統觀點普遍認為，對於許多需要高吞吐量和低延遲的系統來說，共識演算法太慢且成本太高 [Bol11]。這種觀念根本不正確——雖然實現可能很慢，但有許多技巧可以提高性能。分散式共識演算法是 Google 許多關鍵系統的核心，如 [Ana13]、[Bur06]、[Cor12] 和 [Shu13] 中所述，並且它們在實踐中已被證明極其有效。Google 的規模在這裡並不是一個優勢：事實上，我們的規模更像是一個劣勢，因為它引入了兩個主要挑戰：我們的數據集往往很大，我們的系統運行在廣泛的地理距離上。更大的數據集乘以多個副本代表了顯著的計算成本，而更大的地理距離增加了副本之間的延遲，這反過來又降低了性能。

沒有一個「最佳」的分散式共識和狀態機複製演算法在性能方面是最好的，因為性能取決於許多與工作負載、系統的性能目標以及系統如何部署相關的因素。113 雖然以下一些部分介紹了研究，旨在增加對分散式共識可以實現什麼的理解，但所描述的許多系統現在都可用並且正在使用中。

工作負載可以以多種方式變化，理解它們如何變化對於討論性能至關重要。在共識系統的情況下，工作負載可能在以下方面有所不同：

-   吞吐量：在峰值負載下每單位時間提出的提案數量
-   請求類型：改變狀態的操作的比例
-   讀取操作所需的一致性語義
-   請求大小，如果數據負載的大小可以變化的話

部署策略也各不相同。例如：

-   部署是局域網還是廣域網？
-   使用哪種法定人數，大多數進程在哪裡？
-   系統是否使用分片、流水線和批處理？

許多共識系統使用一個傑出的領導者進程，並要求所有請求都轉到這個特殊的節點。如圖 23-6 所示，結果，系統在不同地理位置的客戶端所感知的性能可能會大相徑庭，僅僅因為更遠的節點到領導者進程的來回時間更長。

## Multi-Paxos：詳細的訊息流

**Multi-Paxos** 協議使用一個強大的領導者進程：除非尚未選舉出領導者或發生某些故障，否則它只需要從提議者到法定人數的接受者進行一次來回通訊即可達成共識。使用強大的領導者進程在傳遞的訊息數量方面是最佳的，並且是許多共識協議的典型特徵。

圖 23-7 顯示了一個初始狀態，一個新的提議者正在執行協議的第一個 **Prepare / Promise** 階段。執行此階段會建立一個新的編號視圖或領導者任期。在協議的後續執行中，只要視圖保持不變，第一階段就是不必要的，因為建立該視圖的提議者可以簡單地發送 **Accept** 訊息，一旦收到法定人數的回應（包括提議者本身），就達成共識。

組中的另一個進程可以隨時承擔提議者的角色來提議訊息，但更換提議者有性能成本。它需要額外的來回通訊來執行協議的第一階段，但更重要的是，它可能導致**競爭提議者 (dueling proposers)** 的情況，即提案反覆相互中斷，沒有提案可以被接受，如圖 23-8 所示。由於這種情況是一種**活鎖 (livelock)**，它可以無限期地持續下去。

所有實用的共識系統都解決了這個碰撞問題，通常要麼選舉一個提議者進程，由它來提出系統中的所有提案，要麼使用一個輪流的提議者，為每個進程分配特定的提案時槽。

對於使用領導者進程的系統，領導者選舉過程必須仔細調整，以平衡在沒有領導者時發生的系統不可用性與競爭提議者的風險。實現正確的超時和退避策略很重要。如果多個進程檢測到沒有領導者並同時嘗試成為領導者，那麼很可能沒有一個進程會成功（再次出現競爭提議者）。引入隨機性是最好的方法。例如，**Raft** [Ong14] 有一種深思熟慮的方法來處理領導者選舉過程。

## 擴展讀取密集型工作負載

擴展讀取工作負載通常至關重要，因為許多工作負載都是讀取密集型的。複製資料儲存的優點是數據在多個地方可用，這意味著如果並非所有讀取都需要強一致性，則可以從任何副本讀取數據。這種從副本讀取的技術對於某些應用程式效果很好，例如 Google 的 **Photon** 系統 [Ana13]，它使用分散式共識來協調多個管線的工作。Photon 使用原子性的**比較並設定 (compare-and-set)** 操作進行狀態修改（受到原子暫存器的啟發），這必須是絕對一致的；但讀取操作可以從任何副本提供，因為過時的數據會導致執行額外的工作，但不會導致不正確的結果 [Gup15]。這種權衡是值得的。

為了保證正在讀取的數據是最新的，並且與在執行讀取之前所做的任何更改一致，必須執行以下操作之一：

-   執行一個唯讀的共識操作。
-   從保證是最新的副本讀取數據。在一個使用穩定領導者進程的系統中（許多分散式共識實現都這樣做），領導者可以提供這種保證。
-   使用**法定人數租約 (quorum leases)**，其中一些副本被授予系統中全部或部分數據的租約，允許在犧牲一些寫入性能的代價下進行強一致性的本地讀取。下一節將詳細討論這項技術。

## 法定人數租約 (Quorum Leases)

**法定人數租約 (Quorum leases)** [Mor14] 是最近開發的一種分散式共識性能優化，旨在減少讀取操作的延遲並增加吞吐量。如前所述，在經典 Paxos 和大多數其他分散式共識協議的情況下，執行強一致性讀取（即保證具有最新狀態視圖的讀取）需要一個從法定人數副本讀取的分散式共識操作，或者一個保證已看到所有最近狀態更改操作的穩定領導者副本。在許多系統中，讀取操作的數量遠遠超過寫入操作，因此這種對分散式操作或單個副本的依賴會損害延遲和系統吞吐量。

法定人數租約技術只是將複製資料儲存狀態的某個子集的讀取租約授予法定人數的副本。租約有一個特定的（通常是短暫的）時間段。任何更改該數據狀態的操作都必須得到讀取法定人數中所有副本的確認。如果這些副本中的任何一個變得不可用，則在租約到期之前無法修改數據。

法定人數租約對於讀取密集型工作負載特別有用，其中對特定數據子集的讀取集中在單個地理區域。

## 分散式共識性能與網路延遲

共識系統在提交狀態更改時面臨兩個主要的物理性能限制。一個是網路來回時間，另一個是將數據寫入持久性儲存所需的時間，後者將在稍後進行探討。

網路來回時間因源和目標位置而異，這受到源和目標之間的物理距離以及網路上擁塞程度的影響。在單個資料中心內，機器之間的來回時間應該在毫秒級。在美國境內，典型的來回時間 (RTT) 為 45 毫秒，從紐約到倫敦為 70 毫秒。

在局域網上，共識系統的性能可以與非同步主從複製系統 [Bol11] 相媲美，例如許多傳統資料庫用於複製的系統。然而，分散式共識系統的許多可用性優勢要求副本之間「距離遙遠」，以便處於不同的故障域。

許多共識系統使用 TCP/IP 作為其通訊協議。TCP/IP 是面向連接的，並提供了一些關於訊息 FIFO 排序的強可靠性保證。然而，建立一個新的 TCP/IP 連接需要一個網路來回才能執行建立連接的三向交握，然後才能發送或接收任何數據。TCP/IP 慢啟動最初會限制連接的頻寬，直到其極限被確定。初始 TCP/IP 窗口大小範圍從 4 到 15 KB。

TCP/IP 慢啟動對於組成共識組的進程來說可能不是問題：它們會相互建立連接並保持這些連接打開以供重用，因為它們會頻繁通訊。然而，對於擁有大量客戶端的系統，讓所有客戶端都保持與共識叢集的持久連接可能不切實際，因為打開的 TCP/IP 連接確實會消耗一些資源，例如檔案描述符，此外還會產生 keepalive 流量。對於使用高度分片的、基於共識的資料儲存，包含數千個副本和更多客戶端的應用程式來說，這種開銷可能是一個重要問題。一個解決方案是使用一個區域代理池，如圖 23-9 所示，這些代理持有與共識組的持久 TCP/IP 連接，以避免長距離的設置開銷。代理也可能是封裝分片和負載平衡策略，以及發現叢集成員和領導者的好方法。

## 關於性能的推理：Fast Paxos

**Fast Paxos** [Lam06] 是 Paxos 演算法的一個版本，旨在提高其在廣域網上的性能。使用 Fast Paxos，每個客戶端可以直接向一組接受者的每個成員發送 Propose 訊息，而不是像在 Classic Paxos 或 Multi-Paxos 中那樣通過一個領導者。其思想是用 Fast Paxos 中從客戶端到所有接受者的一次並行訊息發送，來替代 Classic Paxos 中的兩次訊息發送操作：

-   從客戶端到單個提議者的一條訊息
-   從提議者到其他副本的並行訊息發送操作

直觀地看，Fast Paxos 似乎應該總是比 Classic Paxos 快。然而，這並不正確：如果 Fast Paxos 系統中的客戶端到接受者的 RTT（來回時間）很高，而接受者之間的連接很快，那麼我們就用 N 次並行訊息跨越較慢的網路連結（在 Fast Paxos 中）替代了一次訊息跨越較慢的連結加上 N 次並行訊息跨越較快的連結（Classic Paxos）。由於延遲尾部效應，在大多數情況下，一次跨越具有延遲分佈的慢速連結的單個來回比一個法定人數更快（如 [Jun07] 所示），因此，在這種情況下，Fast Paxos 比 Classic Paxos 慢。

許多系統在接受者處將多個操作批處理到單個交易中以提高吞吐量。讓客戶端充當提議者也使得批處理提案變得更加困難。原因在於，提案是獨立到達接受者的，所以您無法以一致的方式對它們進行批處理。

## 穩定的領導者 (Stable Leaders)

我們已經看到 Multi-Paxos 如何選舉一個穩定的領導者來提高性能。Zab [Jun11] 和 Raft [Ong14] 也是出於性能原因而選舉穩定領導者的協議的例子。這種方法可以允許讀取優化，因為領導者擁有最新的狀態，但也有幾個問題：

-   所有改變狀態的操作都必須通過領導者發送，這個要求為那些不位於領導者附近的客戶端增加了網路延遲。
-   領導者進程的傳出網路頻寬是一個系統瓶頸 [Mao08]，因為領導者的 Accept 訊息包含了與任何提案相關的所有數據，而其他訊息只包含對帶有編號交易的確認，沒有數據負載。
-   如果領導者恰好在一台有性能問題的機器上，那麼整個系統的吞吐量將會降低。

幾乎所有以性能為考量設計的分散式共識系統，要麼使用單一穩定領導者模式，要麼使用輪流領導制，其中每個編號的分散式共識演算法都被預先分配給一個副本（通常通過交易 ID 的簡單取模）。使用這種方法的演算法包括 Mencius [Mao08] 和 Egalitarian Paxos [Mor12a]。

在廣域網上，客戶端地理分佈廣泛，共識組的副本位於離客戶端相當近的地方，這樣的領導者選舉會導致客戶端感知的延遲較低，因為他們到最近副本的網路 RTT 平均會比到任意領導者的小。

## 批處理 (Batching)

如《關於性能的推理：Fast Paxos》中所述，**批處理 (Batching)** 可以提高系統吞吐量，但它仍然會讓副本在等待它們發送的訊息的回應時處於閒置狀態。閒置副本所帶來的低效率可以通過**流水線 (pipelining)** 來解決，它允許多個提案同時在途。這種優化與 TCP/IP 的情況非常相似，在 TCP/IP 中，協議試圖使用滑動窗口方法「保持管道滿載」。流水線通常與批處理結合使用。

流水線中的請求批次仍然通過視圖號和交易號進行全域排序，因此這種方法不違反運行複製狀態機所需的全域排序屬性。這種優化方法在 [Bol11] 和 [San11] 中有討論。

## 磁碟存取 (Disk Access)

日誌記錄到持久性儲存是必需的，這樣一個節點在崩潰並返回叢集後，能夠遵守它之前就正在進行的共識交易所做的任何承諾。例如，在 Paxos 協議中，當接受者已經同意了一個具有更高序列號的提案時，它們就不能再同意另一個提案。如果已同意和已提交的提案的細節沒有被記錄到持久性儲存中，那麼一個接受者如果在崩潰並重啟後，可能會違反協議，導致狀態不一致。

將一個條目寫入磁碟上的日誌所需的時間，根據所使用的硬體或虛擬化環境而大不相同，但可能需要一到幾毫秒。

Multi-Paxos 的訊息流在《Multi-Paxos：詳細的訊息流》中進行了討論，但該部分並未顯示協議必須在何處將狀態更改記錄到磁碟。每當一個進程做出它必須遵守的承諾時，就必須進行一次磁碟寫入。在 Multi-Paxos 的性能關鍵的第二階段，這些點發生在接受者響應提案發送 Accepted 訊息之前，以及提議者發送 Accept 訊息之前，因為這個 Accept 訊息也是一個隱含的 Accepted 訊息 [Lam98]。

這意味著單個共識操作的延遲包括以下內容：

-   提議者的一次磁碟寫入
-   向接受者的並行訊息
-   接受者處的並行磁碟寫入
-   返回的訊息

有一種 Multi-Paxos 協議的版本，對於磁碟寫入時間占主導地位的情況很有用：這個變體不認為提議者的 Accept 訊息是一個隱含的 Accepted 訊息。相反，提議者與其他進程並行寫入磁碟，並發送一個明確的 Accept 訊息。延遲於是變成與發送兩條訊息以及法定人數的進程並行執行一次同步磁碟寫入所需的時間成正比。

如果執行一次小的隨機磁碟寫入的延遲在 10 毫秒的數量級，那麼共識操作的速率將被限制在約每秒 100 次。這些時間假設網路來回時間可以忽略不計，並且提議者與接受者並行執行其日誌記錄。

正如我們已經看到的，分散式共識演算法通常被用作建立複製狀態機的基礎。RSM 也需要為恢復目的保留交易日誌（原因與任何資料儲存相同）。共識演算法的日誌和 RSM 的交易日誌可以合併為單個日誌。合併這些日誌避免了需要在磁碟上兩個不同物理位置之間不斷交替寫入的需要 [Bol11]，從而減少了花在尋道操作上的時間。磁碟可以每秒維持更多的操作，因此，整個系統可以執行更多的交易。

在資料儲存中，磁碟除了維護日誌外還有其他用途：系統狀態通常也維護在磁碟上。日誌寫入必須直接刷新到磁碟，但狀態更改的寫入可以寫入記憶體快取，稍後再刷新到磁碟，並重新排序以使用最有效的排程 [Bol11]。

另一個可能的優化是在提議者處將多個客戶端操作批處理成一個操作（[Ana13]、[Bol11]、[Cha07]、[Jun11]、[Mao08]、[Mor12a]）。這將磁碟日誌記錄和網路延遲的固定成本分攤到更多的操作上，從而提高吞吐量。

# 部署基於分散式共識的系統

系統設計師在部署基於共識的系統時必須做出的最關鍵的決定，涉及要部署的副本數量和這些副本的位置。

## 副本數量 (Number of Replicas)

一般來說，基於共識的系統使用**多數法定人數 (majority quorums)** 運作，即一組 `2f + 1` 個副本可以容忍 `f` 個故障（如果需要**拜占庭容錯 (Byzantine fault tolerance)**，即系統能抵抗副本返回不正確結果的情況，則 `3f + 1` 個副本可以容忍 `f` 個故障 [Cas99]）。對於非拜占庭故障，可以部署的最小副本數量是三個——如果部署了兩個，那麼對任何進程的故障都沒有容忍度。三個副本可以容忍一個故障。大多數系統停機時間是計劃性維護的結果 [Ken12]：三個副本允許系統在一個副本因維護而停機時正常運行（假設剩餘的兩個副本能夠以可接受的性能處理系統負載）。

如果在維護窗口期間發生意外故障，那麼共識系統將變得不可用。共識系統的不可用性通常是不可接受的，因此應該運行五個副本，允許系統在最多兩個故障的情況下運行。如果一個共識系統中的五個副本中有四個仍然存在，則不一定需要干預，但如果只剩下三個，則應增加一個或兩個額外的副本。

如果一個共識系統失去了如此多的副本以至於無法形成法定人數，那麼該系統理論上處於不可恢復的狀態，因為至少一個缺失副本的持久日誌無法訪問。如果沒有法定人數剩下，那麼可能已經做出了一個只有缺失的副本看到的決定。管理員可能能夠強制更改組成員並添加新的副本，從現有副本中趕上進度以便繼續，但數據丟失的可能性始終存在——這種情況應盡可能避免。

在災難中，管理員必須決定是執行這樣的強制性重新配置，還是等待一段時間讓帶有系統狀態的機器變得可用。在做出此類決定時，對系統日誌的處理（除了監控）變得至關重要。理論論文經常指出，共識可以用於構建一個複製日誌，但未能討論如何處理可能失敗和恢復（從而錯過一些共識決定序列）或因緩慢而落後的副本。為了維護系統的穩健性，這些副本趕上進度是很重要的。

複製日誌在分散式共識理論中並不總是一等公民，但它是生產系統中一個非常重要的方面。Raft 描述了一種管理複製日誌一致性的方法 [Ong14]，明確定義了如何填補副本ログ中的任何空白。如果一個五實例的 Raft 系統失去了除其領導者之外的所有成員，領導者仍然保證對所有已提交的決定有完整的了解。另一方面，如果缺失的多數成員包括領導者，則無法對剩餘副本的更新程度做出強有力的保證。

系統中不需要組成法定人數的副本數量與性能之間存在關係：少數較慢的副本可能會落後，從而允許性能較好的副本組成的法定人數運行得更快（只要領導者表現良好）。如果副本性能差異很大，那麼每次故障都可能降低整個系統的性能，因為需要慢的離群值來組成法定人數。系統能容忍的故障或落後副本越多，整個系統的性能可能就越好。

在管理副本時也應考慮成本問題：每個副本都使用昂貴的計算資源。如果所討論的系統是單個進程叢集，運行副本的成本可能不是一個大的考慮因素。然而，對於像 Photon [Ana13] 這樣的系統，副本的成本可能是一個嚴重的考慮因素，它使用分片配置，其中每個分片都是一個運行共識演算法的完整進程組。隨著分片數量的增長，每個額外副本的成本也隨之增長，因為必須向系統中添加與分片數量相等的進程數量。

因此，任何系統的副本數量決定都是以下因素之間的權衡：

-   可靠性的需求
-   影響系統的計劃性維護的頻率
-   風險
-   性能
-   成本

這個計算對每個系統都會有所不同：系統有不同的可用性服務水平目標；一些組織比其他組織更定期地執行維護；組織使用的硬體成本、品質和可靠性各不相同。

## 副本位置 (Location of Replicas)

關於在哪裡部署組成共識叢集的進程的決定，是基於兩個因素：系統應處理的故障域與系統的延遲要求之間的權衡。在決定副本位置時，有多個複雜的問題在起作用。

**故障域 (failure domain)** 是一個系統中可能因單一故障而變得不可用的組件集合。故障域的例子包括：

-   一台物理機器
-   由單一電源供應的資料中心的一個機架
-   由一件網路設備服務的資料中心中的幾個機架
-   可能因光纖電纜切斷而變得不可用的資料中心
-   可能都受到單一自然災害（如颶風）影響的單一地理區域中的一組資料中心

一般來說，隨著副本之間距離的增加，副本之間的來回時間以及系統能夠容忍的故障規模也會增加。對於大多數共識系統，增加副本之間的來回時間也會增加操作的延遲。

延遲的重要性，以及在特定域中生存故障的能力，都非常依賴於系統。一些共識系統架構不要求特別高的吞吐量或低延遲：例如，一個為了為高可用性服務提供組成員和領導者選舉服務而存在的共識系統，可能負載不重，如果共識交易時間只是領導者租約時間的一小部分，那麼其性能就不是關鍵。以批次為導向的系統也較少受到延遲的影響：可以增加操作批次大小以提高吞吐量。

持續增加系統可以承受其損失的故障域的大小並不總是有意義的。例如，如果使用共識系統的所有客戶端都在一個特定的故障域內運行（比如，紐約地區），而在更廣泛的地理區域部署基於分散式共識的系統將使其在該故障域中斷期間（比如，颶風桑迪）仍能提供服務，這值得嗎？可能不值得，因為系統的客戶端也會宕機，所以系統將看不到任何流量。在延遲、吞吐量和計算資源方面的額外成本將不會帶來任何好處。

在決定副本位置時，您應該考慮到災難恢復：在儲存關鍵數據的系統中，共識副本也基本上是系統數據的在線副本。然而，當關鍵數據處於危險之中時，即使在部署在幾個不同故障域的堅實的基於共識的系統中，定期備份快照到別處也很重要。有兩個您永遠無法逃脫的故障域：軟體本身，以及系統管理員的人為錯誤。軟體中的錯誤可能在不尋常的情況下出現並導致數據丟失，而系統配置錯誤可能產生類似的效果。操作人員也可能出錯，或進行破壞導致數據丟失。

在做出關於副本位置的決定時，請記住，最重要的性能衡量標準是客戶端的感知：理想情況下，從客戶端到共識系統副本的網路來回時間應該最小化。在廣域網上，像 Mencius 或 Egalitarian Paxos 這樣的無領導者協議可能具有性能優勢，特別是如果應用程式的一致性約束意味著可以在任何系統副本上執行唯讀操作而無需執行共識操作。

## 容量與負載平衡 (Capacity and Load Balancing)

在設計部署時，您必須確保有足夠的容量來應對負載。在**分片部署 (sharded deployments)** 的情況下，您可以通過調整分片的數量來調整容量。然而，對於可以從不是領導者的共識組成員讀取的系統，您可以通過增加更多副本來增加讀取容量。增加更多副本是有成本的：在使用強領導者的演算法中，增加副本會給領導者進程帶來更多負載，而在對等協議中，增加副本會給所有進程帶來更多負載。然而，如果有充足的寫入操作容量，但讀取密集型工作負載正在給系統帶來壓力，那麼增加副本可能是最好的方法。

應該注意的是，在多數法定人數系統中增加一個副本可能會在一定程度上降低系統的可用性（如圖 23-10 所示）。Zookeeper 或 Chubby 的典型部署使用五個副本，因此多數法定人數需要三個副本。如果兩個副本（即 40%）不可用，系統仍將取得進展。如果有六個副本，法定人數需要四個副本：只有 33% 的副本可以不可用，系統才能保持活動狀態。

因此，當增加第六個副本時，關於故障域的考慮就更加適用了：如果一個組織有五個資料中心，並且通常運行有五個進程的共識組，每個資料中心一個，那麼失去一個資料中心仍然會在每個組中留下一個備用副本。如果在五個資料中心之一部署了第六個副本，那麼該資料中心的中斷將移除組中的兩個備用副本，從而將容量減少 33%。

如果客戶端在某個特定地理區域密集，最好將副本放置在靠近客戶端的地方。然而，確切地決定在哪裡放置副本可能需要圍繞負載平衡和系統如何處理過載進行一些仔細的思考。如圖 23-11 所示，如果一個系統只是將客戶端讀取請求路由到最近的副本，那麼集中在一個區域的大量負載尖峰可能會壓垮最近的副本，然後是次近的副本，依此類推——這就是級聯故障（見《處理級聯故障》）。這種類型的過載通常可能由於批次作業的開始而發生，特別是如果幾個作業同時開始的話。

我們已經看到了許多分散式共識系統出於提高性能的原因而使用領導者進程。然而，重要的是要理解領導者副本將使用更多的計算資源，特別是傳出網路容量。這是因為領導者發送的提案訊息包含了提議的數據，而副本發送的訊息較小，通常只包含對特定共識交易 ID 的同意。運行具有大量進程的高度分片共識系統的組織可能會發現，有必要確保不同分片的領導者進程在不同資料中心之間相對均勻地平衡。這樣做可以防止整個系統因單個資料中心的傳出網路容量而成為瓶頸，並使整個系統的容量大得多。

在多個資料中心部署共識組的另一個缺點是（如圖 23-11 所示），如果托管領導者的資料中心遭受廣泛的故障（例如，電力、網路設備故障或光纖切斷），系統可能會發生非常極端的變化。如圖 23-12 所示，在這種故障情境下，所有的領導者都應該故障轉移到另一個資料中心，要麼平均分配，要麼集體轉移到一個資料中心。無論哪種情況，其他兩個資料中心之間的連結將突然接收到來自該系統的更多網路流量。這將是一個發現該連結容量不足的不合時宜的時刻。

然而，這種類型的部署很可能是一個系統中對領導者如何選擇有影響的自動化過程的無意結果。例如：

-   如果領導者位於離客戶端最近的地方，客戶端將為任何通過領導者處理的操作體驗到更好的延遲。一個試圖將領導者安置在大量客戶端附近的演算法可以利用這一點。
-   一個演算法可能會試圖將領導者安置在性能最好的機器上。這種方法的一個陷阱是，如果三個資料中心之一擁有更快的機器，那麼不成比例的流量將被發送到該資料中心，導致如果該資料中心離線，流量會發生極端變化。為避免此問題，演算法在選擇機器時還必須考慮到分佈平衡與機器能力的關係。
-   一個領導者選舉演算法可能偏愛運行時間較長的進程。如果軟體發布是按資料中心進行的，那麼運行時間較長的進程很可能與位置相關。

### 法定人數組成 (Quorum composition)

在確定共識組中副本的位置時，重要的是要考慮地理分佈（或者更準確地說，副本之間的網路延遲）對組性能的影響。

一種方法是盡可能均勻地分散副本，所有副本之間的 RTT 相似。在所有其他因素（如工作負載、硬體和網路性能）相同的情況下，這種安排應該會導致所有地區的性能相當一致，無論組領導者位於何處（或者，如果使用無領導者協議，則對於共識組的每個成員）。

地理位置會使這種方法大大複雜化。這對於洲內與跨太平洋和跨大西洋的流量尤其如此。考慮一個橫跨北美和歐洲的系統：不可能將副本放置在彼此等距的地方，因為跨大西洋的流量總會有比洲內流量更長的延遲。無論如何，來自一個地區的交易都需要進行一次跨大西洋的來回才能達成共識。

然而，如圖 23-13 所示，為了盡可能均勻地分配流量，系統設計師可能會選擇設置五個副本，其中兩個副本大致位於美國中部，一個在東海岸，兩個在歐洲。這樣的分配意味著，在平均情況下，可以在北美達成共識而無需等待來自歐洲的回應，或者從歐洲可以僅通過與東海岸副本交換訊息來達成共識。東海岸副本起到了一種類似於關鍵樞紐的作用，兩個可能的法定人數在此重疊。

如圖 23-14 所示，失去這個副本意味著系統延遲可能會發生巨大變化：延遲不再主要受美國中部到東海岸的 RTT 或歐盟到東海岸的 RTT 的影響，而是基於歐盟到中部的 RTT，這比歐盟到東海岸的 RTT 高出約 50%。最近的可能法定人數之間的地理距離和網路 RTT 大大增加。

當應用於由成員之間 RTT 非常不同的副本組成的組時，這種情境是簡單多數法定人數的一個關鍵弱點。在這種情況下，**分層法定人數 (hierarchical quorum)** 方法可能有用。如圖 23-15 所示，可以將九個副本部署在三個各含三個副本的組中。法定人數可以由多數組形成，如果一個組的多數成員可用，則該組可以包含在法定人數中。這意味著中央組中的一個副本可能會丟失，而不會對整個系統性能產生大的影響，因為中央組仍然可以用其三個副本中的兩個對交易進行投票。

然而，運行更多數量的副本存在資源成本。在一個具有讀取密集型工作負載且主要可由副本滿足的高度分片系統中，我們可能會通過使用較少的共識組來減輕這種成本。這樣的策略意味著系統中的進程總數可能不會改變。

# 監控分散式共識系統

正如我們已經看到的，分散式共識演算法是 Google 許多關鍵系統的核心（[Ana13]、[Bur06]、[Cor12]、[Shu13]）。所有重要的生產系統都需要監控，以便檢測中斷或問題並進行故障排除。經驗告訴我們，分散式共識系統的某些特定方面值得特別關注。這些是：

為了理解系統性能並幫助排除性能問題，您可能還需要監控以下內容：

-   提案接受的延遲分佈
-   在不同位置的系統部分之間觀察到的網路延遲分佈
-   接受者在持久日誌記錄上花費的時間
-   系統中每秒接受的總位元組數

# 結論 (Conclusion)

我們探討了分散式共識問題的定義，並介紹了一些基於分散式共識的系統架構模式，同時也檢視了性能特點和圍繞基於分散式共識系統的一些操作性問題。

我們在本章中刻意避免了對特定演算法、協議或實現的深入討論。分散式協調系統及其底層技術正在迅速發展，這些資訊會很快過時，不像這裡討論的基礎知識。然而，這些基礎知識，連同本章中引用的文章，將使您能夠使用當今可用的分散式協調工具以及未來的軟體。

如果您從本章中什麼都記不住，請記住分散式共識可以用來解決哪些類型的問題，以及當使用心跳等臨時方法代替分散式共識時可能出現的問題類型。每當您看到領導者選舉、關鍵共享狀態或分散式鎖定時，請考慮分散式共識：任何較差的方法都是一顆定時炸彈，等待在您的系統中爆炸。

112 Kyle Kingsbury 撰寫了一系列關於分散式系統正確性的廣泛文章，其中包含這些類型的資料儲存中許多意外和不正確行為的例子。請參閱 https://aphyr.com/tags/jepsen 。

113 特別是，原始 Paxos 演算法的性能並不理想，但多年來已得到極大改進。