# 處理級聯故障 (Addressing Cascading Failures)

作者：Mike Ulrich

> 如果一開始你沒有成功，就以指數級退避。
> Dan Sandler, Google 軟體工程師

> 為什麼人們總是忘記需要加一點抖動 (jitter)？
> Ade Oshineye, Google 開發者倡導者

**級聯故障 (cascading failure)** 是一種由於正回饋而隨時間增長的故障。107 當整個系統的一部分發生故障，增加了系統其他部分發生故障的可能性時，就會發生這種情況。例如，服務的單個副本可能因過載而失敗，從而增加了剩餘副本的負載，並增加了它們失敗的可能性，導致多米諾骨牌效應，使服務的所有副本都宕機。

我們將在本章中以《莎士比亞：一個範例服務》中討論的莎士比亞搜尋服務為例。其生產配置可能看起來像圖 22-1。

## 級聯故障的原因及避免設計 (Causes of Cascading Failures and Designing to Avoid Them)

深思熟慮的系統設計應考慮到一些導致大多數級聯故障的典型情境。

### 伺服器過載 (Server Overload)

級聯故障最常見的原因是過載。這裡描述的大多數級聯故障要麼直接由伺服器過載引起，要麼是此情境的延伸或變體。

假設叢集 A 的前端正在處理每秒 1,000 個請求 (QPS)，如圖 22-2 所示。

如果叢集 B 失敗（圖 22-3），對叢集 A 的請求增加到 1,200 QPS。A 中的前端無法處理 1,200 QPS 的請求，因此開始耗盡資源，導致它們崩潰、錯過截止日期或以其他方式行為不當。結果，A 中成功處理的請求速率遠低於 1,000 QPS。

有用工作完成率的這種降低可能會擴散到其他故障域，並可能在全球範圍內擴散。例如，一個叢集中的局部過載可能導致其伺服器崩潰；作為回應，負載平衡控制器將請求發送到其他叢集，使它們的伺服器過載，從而導致全服務範圍的過載故障。這些事件的發生可能不需要很長時間（例如，幾分鐘），因為所涉及的負載平衡器和任務調度系統可能行動非常迅速。

### 資源耗盡 (Resource Exhaustion)

資源耗盡可能導致更高的延遲、更高的錯誤率或替換為品質較低的結果。這些實際上是資源耗盡所期望的效果：當負載增加到伺服器無法處理的程度時，最終需要有所犧牲。

根據伺服器中耗盡的資源類型以及伺服器的構建方式，資源耗盡可能使伺服器效率降低或導致伺服器崩潰，從而促使負載平衡器將資源問題分配給其他伺服器。當這種情況發生時，成功處理的請求速率可能會下降，並可能使叢集或整個服務陷入級聯故障。

不同類型的資源可能會被耗盡，從而對伺服器產生不同的影響。

#### CPU

如果 CPU 不足以處理請求負載，通常所有請求都會變慢。這種情況可能導致各種次要效應，包括：

#### 記憶體 (Memory)

如果沒有其他問題，更多的在途請求會從分配請求、回應和 RPC 物件中消耗更多的 RAM。記憶體耗盡可能導致以下效應：

#### 執行緒 (Threads)

執行緒饑餓可能直接導致錯誤或健康檢查失敗。如果伺服器根據需要添加執行緒，執行緒開銷可能會使用過多的 RAM。在極端情況下，執行緒饑餓也可能導致您用完進程 ID。

#### 檔案描述符 (File descriptors)

檔案描述符耗盡可能導致無法初始化網路連接，這反過來又可能導致健康檢查失敗。

#### 資源之間的依賴關係

請注意，許多這些資源耗盡情境是相互關聯的——一個經歷過載的服務通常會有一系列次要症狀，這些症狀看起來可能像根本原因，使得除錯變得困難。

例如，想像以下情境：

-   一個 Java 前端的**垃圾回收 (garbage collection, GC)** 參數調整不佳。
-   在高（但預期內）負載下，前端因 GC 而耗盡 CPU。
-   CPU 耗盡減慢了請求的完成速度。
-   增加的進行中請求數量導致需要使用更多 RAM 來處理請求。
-   由請求引起的記憶體壓力，加上為整個前端進程分配的固定記憶體，使得可用於快取的 RAM 減少。
-   減少的快取大小意味著快取中的條目更少，此外還有較低的命中率。
-   快取未命中的增加意味著更多的請求會落到後端進行服務。
-   後端反過來又耗盡了 CPU 或執行緒。
-   最後，CPU 的缺乏導致基本的健康檢查失敗，從而引發級聯故障。

在像前述情境一樣複雜的情況下，在服務中斷期間完全診斷出因果鏈是不太可能的。可能很難確定後端崩潰是由於前端快取率下降引起的，特別是如果前端和後端組件由不同的所有者負責。

## 服務不可用 (Service Unavailability)

資源耗盡可能導致伺服器崩潰；例如，當分配給容器的 RAM 過多時，伺服器可能會崩潰。一旦幾台伺服器因過載而崩潰，剩餘伺服器上的負載可能會增加，導致它們也崩潰。問題往往會像雪球一樣越滾越大，很快所有伺服器都開始**崩潰循環 (crash-loop)**。通常很難擺脫這種情況，因為伺服器一上線，就會被極高率的請求轟炸，幾乎立即失敗。

例如，如果一個服務在 10,000 QPS 時是健康的，但在 11,000 QPS 時由於崩潰而開始級聯故障，將負載降至 9,000 QPS 幾乎肯定無法停止崩潰。這是因為該服務將以減少的容量處理增加的需求；通常只有一小部分伺服器足夠健康以處理請求。能夠健康的伺服器比例取決於幾個因素：系統啟動任務的速度、二進位檔案能夠以全容量開始服務的速度，以及一個剛啟動的任務能夠在負載下存活多長時間。在這個例子中，如果 10% 的伺服器足夠健康以處理請求，請求率需要降至約 1,000 QPS，系統才能穩定和恢復。

同樣，伺服器可能對負載平衡層顯得不健康，導致負載平衡容量減少：伺服器可能進入「跛腳鴨」狀態（參見《穩健處理不健康任務的方法：跛腳鴨狀態》）或在不崩潰的情況下健康檢查失敗。其效果可能與崩潰非常相似：更多伺服器顯得不健康，健康的伺服器在變得不健康之前傾向於接受請求的時間非常短，並且更少的伺服器參與處理請求。

避免那些曾提供過錯誤的伺服器的負載平衡策略可能會使問題進一步惡化——一些後端提供了一些錯誤，因此它們對服務的可用容量沒有貢獻。這增加了剩餘伺服器的負載，開始了雪球效應。

# 預防伺服器過載 (Preventing Server Overload)

以下列表按大致的優先順序介紹了避免伺服器過載的策略：

-   在**反向代理 (reverse proxies)** 處，通過 IP 位址等標準限制請求量，以減輕拒絕服務攻擊和濫用客戶端的嘗試。
-   在**負載平衡器 (load balancers)** 處，當服務進入全域過載時丟棄請求。根據服務的性質和複雜性，這種速率限制可以是無差別的（「丟棄所有超過 X 個請求/秒的流量」）或更具選擇性的（「丟棄不是來自最近與服務互動過的用戶的請求」或「丟棄低優先級操作（如背景同步）的請求，但繼續服務互動式用戶會話」）。
-   在**個別任務**處，以防止負載平衡的隨機波動壓垮伺服器。

容量規劃降低了觸發級聯故障的機率，但它不足以保護服務免受級聯故障的影響。當您在計劃內或計劃外事件中失去基礎設施的主要部分時，任何數量的容量規劃都可能不足以防止級聯故障。負載平衡問題、網路分區或意外的流量增加都可能產生超出計劃的高負載區域。一些系統可以根據需求增加您服務的任務數量，這可能會防止過載；然而，仍然需要適當的容量規劃。

## 佇列管理 (Queue Management)

大多數**每個請求一個執行緒 (thread-per-request)** 的伺服器在執行緒池前使用一個佇列來處理請求。請求進來，它們待在佇列中，然後執行緒從佇列中取出請求並執行實際工作（伺服器所需的任何操作）。通常，如果佇列已滿，伺服器將拒絕新請求。

如果給定任務的請求率和延遲是恆定的，就沒有理由對請求進行排隊：應該有恆定數量的執行緒被佔用。在這種理想化的情境下，只有當傳入請求的穩態速率超過伺服器處理請求的速率時，請求才會被排隊，這會導致執行緒池和佇列的飽和。

排隊的請求會消耗記憶體並增加延遲。例如，如果佇列大小是執行緒數量的 10 倍，處理執行緒上的請求的時間是 100 毫秒。如果佇列已滿，那麼處理一個請求將需要 1.1 秒，其中大部分時間都花在佇列上。

對於一個隨著時間推移流量相當穩定的系統，相對於執行緒池大小，擁有較小的佇列長度（例如，50% 或更少）通常更好，這會導致伺服器在無法維持傳入請求速率時提早拒絕請求。例如，Gmail 經常使用無佇列的伺服器，而是在執行緒滿時依賴於故障轉移到其他伺服器任務。在光譜的另一端，對於流量模式劇烈波動的「突發性」負載的系統，基於當前使用的執行緒數量、每個請求的處理時間以及突發的大小和頻率的佇列大小可能會做得更好。

## 減載與優雅降級 (Load Shedding and Graceful Degradation)

**減載 (Load shedding)** 是指隨著伺服器接近過載條件時，通過丟棄流量來減少一部分負載。其目標是防止伺服器耗盡 RAM、健康檢查失敗、以極高的延遲提供服務，或任何其他與過載相關的症狀，同時仍然盡可能多地做有用的工作。

減載的一個直接方法是基於 CPU、記憶體或佇列長度進行**每個任務的節流 (per-task throttling)**；如《佇列管理》中所討論的，限制佇列長度是這種策略的一種形式。例如，一個有效的方法是當有超過給定數量的客戶端請求在途時，向任何傳入的請求返回 HTTP 503（服務不可用）。

將排隊方法從標準的**先進先出 (first-in, first-out, FIFO)** 改為**後進先出 (last-in, first-out, LIFO)**，或使用**受控延遲 (controlled delay, CoDel)** 演算法 [Nic12] 或類似方法，可以通過移除那些不太可能值得處理的請求來減少負載 [Mau15]。如果一個用戶的網頁搜尋因為一個 RPC 已經在佇列中排了 10 秒而變慢，那麼用戶很可能已經放棄並刷新了他們的瀏覽器，發出了另一個請求：回應第一個請求沒有意義，因為它將被忽略！這種策略與在整個堆疊中傳播 RPC 截止日期（在《延遲與截止日期》中描述）相結合時效果很好。

更複雜的方法包括識別客戶端以更具選擇性地決定丟棄哪些工作，或挑選更重要的請求並優先處理。這種策略更可能被共享服務所需要。

**優雅降級 (Graceful degradation)** 將減載的概念更進一步，通過減少需要執行的工作量。在某些應用中，可以通過降低回應品質來顯著減少所需的工作量或時間。例如，一個搜尋應用程式在過載時可能只搜尋儲存在記憶體快取中的數據子集，而不是完整的磁碟資料庫，或者使用一個不太準確（但更快）的排名演算法。

在為您的服務評估減載或優雅降級選項時，請考慮以下幾點：

-   您應該使用哪些指標來確定何時應該啟動減載或優雅降級（例如，CPU 使用率、延遲、佇列長度、使用的執行緒數，您的服務是自動進入降級模式還是需要手動干預）？
-   當伺服器處於降級模式時應該採取什麼行動？
-   減載和優雅降級應該在哪一層實現？在堆疊的每一層都實現這些策略是否有意義，還是有一個高層級的**瓶頸點 (choke-point)** 就足夠了？

在您評估選項和部署時，請記住以下幾點：

-   優雅降級不應頻繁觸發——通常是在容量規劃失敗或意外負載轉移的情況下。保持系統簡單易懂，特別是如果不經常使用的話。
-   記住，您從不使用的程式碼路徑就是（通常）不起作用的程式碼路徑。在穩態操作中，不會使用優雅降級模式，這意味著您對此模式及其任何怪癖的操作經驗將少得多，從而增加了風險級別。您可以通過定期運行一小部分接近過載的伺服器來確保優雅降級保持工作，以鍛鍊這條程式碼路徑。
-   當太多伺服器進入這些模式時，進行監控和警報。
-   複雜的減載和優雅降級本身也可能引起問題——過度的複雜性可能導致伺服器在不希望的時候進入降級模式，或在不希望的時候進入回饋循環。設計一種方法，以便在需要時快速關閉複雜的優雅降級或調整參數。將此配置儲存在一個一致的系統中，例如 Chubby，每個伺服器都可以觀察其變化，這可以提高部署速度，但也會引入其自身的同步失敗風險。

## 重試 (Retries)

假設與後端通訊的前端程式碼天真地實現了重試。它在遇到失敗後重試，並將每個邏輯請求的後端 RPC 數量上限設為 10。考慮在前端使用 gRPC in Go 的這段程式碼：

這個系統可以通過以下方式級聯：

-   假設我們的後端有一個已知的每個任務 10,000 QPS 的限制，超過該點後，所有進一步的請求都會被拒絕，以嘗試優雅降級。
-   前端以 10,100 QPS 的恆定速率調用 `MakeRequest`，使後端過載 100 QPS，後端會拒絕這些請求。
-   這 100 個失敗的 QPS 會在 `MakeRequest` 中每 1,000 毫秒重試一次，並且很可能會成功。但重試本身也增加了發送到後端的請求，後端現在接收到 10,200 QPS——其中 200 QPS 因過載而失敗。
-   重試的量會增長：第一秒的 100 QPS 重試導致 200 QPS，然後是 300 QPS，依此類推。越來越少的請求能夠在第一次嘗試時成功，因此作為對後端請求的一部分，執行的有用工作越來越少。
-   如果後端任務無法處理負載的增加——這會消耗後端的檔案描述符、記憶體和 CPU 時間——它可能會在請求和重試的巨大負載下崩潰。這次崩潰然後將其接收的請求重新分配到剩餘的後端任務中，從而進一步使這些任務過載。

這裡做了一些簡化假設來闡述這個情境，110 但重點仍然是重試可以使系統不穩定。請注意，臨時的負載尖峰和使用量的緩慢增長都可能導致這種效應。

即使對 `MakeRequest` 的調用率下降到崩潰前的水平（例如 9,000 QPS），根據返回失敗對後端的成本有多大，問題可能不會消失。這裡有兩個因素在起作用：

-   如果後端花費大量資源處理最終會因過載而失敗的請求，那麼重試本身可能使後端保持在過載模式。
-   後端伺服器本身可能不穩定。重試可以放大在《伺服器過載》中看到的效果。

如果這兩個條件中的任何一個為真，為了擺脫這次中斷，您必須大幅減少或消除前端的負載，直到重試停止且後端穩定下來。

這種模式已導致了幾次級聯故障，無論是前端和後端通過 RPC 訊息通訊，還是「前端」是發出 `XmlHttpRequest` 調用到端點並在失敗時重試的客戶端 JavaScript 程式碼，或者是源於一個在遇到失敗時積極重試的離線同步協議。

在發出自動重試時，請記住以下考慮事項：

-   《預防伺服器過載》中描述的大多數後端保護策略都適用。特別是，測試系統可以突顯問題，而優雅降級可以減少重試對後端的影響。
-   在安排重試時，始終使用**隨機化指數退避 (randomized exponential backoff)**。另請參見 AWS 架構部落格中的「指數退避與抖動 (Exponential Backoff and Jitter)」[Bro15]。如果重試在重試窗口內不是隨機分佈的，一個小的擾動（例如，網路瞬間故障）可能導致重試波紋在同一時間安排，然後它們可以自我放大 [Flo94]。
-   限制每個請求的重試次數。不要無限期地重試給定的請求。
-   考慮設定一個全伺服器範圍的重試預算。例如，在一個進程中每分鐘只允許 60 次重試，如果超過了重試預算，就不要重試；直接讓請求失敗。這種策略可以控制重試效應，並且可能是一個容量規劃失敗導致一些查詢被丟棄和一個全域級聯故障之間的區別。
-   從整體上考慮服務，並決定您是否真的需要在給定的層級執行重試。特別是，避免通過在多個層級發出重試來放大重試：最高層的單個請求可能產生的嘗試次數，可能大到等於每一層到最低層的嘗試次數的乘積。如果資料庫因為過載而無法服務請求，而後端、前端和 JavaScript 層都發出 3 次重試（4 次嘗試），那麼單個用戶操作可能會對資料庫產生 64 次嘗試（4^3）。當資料庫因為過載而返回這些錯誤時，這種行為是不可取的。
-   使用清晰的回應代碼，並考慮應如何處理不同的失敗模式。例如，區分可重試和不可重試的錯誤條件。不要在客戶端重試永久性錯誤或格式錯誤的請求，因為兩者都不會成功。在過載時返回一個特定的狀態，以便客戶端和其他層退避並且不重試。

在緊急情況下，可能不清楚中斷是由於不良的重試行為造成的。重試率的圖表可能是不良重試行為的一個跡象，但可能會被誤認為是症狀而不是複合原因。在緩解方面，這是容量不足問題的一個特例，另外需要注意的是，您必須要麼修復重試行為（通常需要程式碼推送），要麼大幅減少負載，要麼完全切斷請求。

## 延遲與截止日期 (Latency and Deadlines)

當一個前端向一個後端伺服器發送一個 RPC 時，前端會消耗資源等待回應。**RPC 截止日期 (RPC deadlines)** 定義了一個請求在前端放棄之前可以等待多長時間，從而限制了後端可能消耗前端資源的時間。

### 挑選一個截止日期

設定一個截止日期通常是明智的。不設定截止日期或設定一個極高的截止日期，可能會導致早已過去的短期問題繼續消耗伺服器資源，直到伺服器重啟。

高的截止日期可能導致在堆疊的較低層級出現問題時，較高層級的資源消耗。短的截止日期可能導致一些較昂貴的請求持續失敗。平衡這些約束以挑選一個好的截止日期可能是一門藝術。

### 錯過截止日期

許多級聯中斷的一個共同主題是，伺服器花費資源處理那些在客戶端將會超過其截止日期的請求。結果，資源被花費了，卻沒有取得任何進展：對於 RPC，遲交的作業是不算分的。

假設一個 RPC 有一個由客戶端設定的 10 秒截止日期。伺服器非常過載，結果，它需要 11 秒才能從佇列移動到執行緒池。此時，客戶端已經放棄了該請求。在大多數情況下，伺服器嘗試處理此請求是不明智的，因為它將做的工作不會得到任何回報——客戶端不在乎截止日期過後伺服器做了什麼工作，因為它已經放棄了該請求。

如果處理一個請求是分多個階段執行的（例如，有幾個回呼和 RPC 調用），伺服器應該在嘗試對請求進行任何更多工作之前，在每個階段檢查剩餘的截止日期。例如，如果一個請求被分成解析、後端請求和處理階段，那麼在每個階段之前檢查是否有足夠的時間來處理請求可能是有意義的。

### 截止日期傳播 (Deadline propagation)

伺服器不應該在向後端發送 RPC 時自己發明一個截止日期，而應該採用**截止日期傳播 (deadline propagation)**。

通過截止日期傳播，截止日期在堆疊的高層（例如，在前端）設定。從初始請求發出的 RPC 樹都將具有相同的絕對截止日期。例如，如果伺服器 A 選擇了一個 30 秒的截止日期，並在向伺服器 B 發送 RPC 之前處理了 7 秒的請求，那麼從 A 到 B 的 RPC 將有 23 秒的截止日期。如果伺服器 B 花了 4 秒來處理請求並向伺服器 C 發送一個 RPC，那麼從 B 到 C 的 RPC 將有 19 秒的截止日期，依此類推。理想情況下，請求樹中的每個伺服器都實現了截止日期傳播。

如果沒有截止日期傳播，可能會出現以下情境：

-   伺服器 A 向伺服器 B 發送一個帶有 10 秒截止日期的 RPC。
-   伺服器 B 花了 8 秒開始處理請求，然後向伺服器 C 發送一個 RPC。
-   如果伺服器 B 使用截止日期傳播，它應該設定一個 2 秒的截止日期，但假設它反而為對伺服器 C 的 RPC 使用了一個硬編碼的 20 秒截止日期。
-   伺服器 C 在 5 秒後從其佇列中取出請求。

如果伺服器 B 使用了截止日期傳播，伺服器 C 可以立即放棄該請求，因為 2 秒的截止日期已經超過。然而，在這種情況下，伺服器 C 處理請求時認為它還有 15 秒的時間，但它並沒有做有用的工作，因為從伺服器 A 到伺服器 B 的請求已經超過了其截止日期。

您可能希望將傳出的截止日期稍微減少一點（例如，幾百毫秒），以考慮網路傳輸時間和客戶端的後處理。

還要考慮為傳出的截止日期設定一個上限。您可能希望限制伺服器等待對非關鍵後端的傳出 RPC 或對通常在短時間內完成的後端的 RPC 的時間。但是，請務必了解您的流量混合情況，因為否則您可能會無意中使特定類型的請求一直失敗（例如，具有大負載的請求，或需要回應大量計算的請求）。

在某些例外情況下，伺服器可能希望在截止日期過後繼續處理請求。例如，如果一個伺服器收到一個涉及執行一些昂貴的追趕操作並定期對追趕進度進行檢查點的請求，那麼最好只在寫入檢查點之後，而不是在昂貴的操作之後檢查截止日期。

### 取消傳播 (Cancellation propagation)

**傳播取消 (Propagating cancellations)** 通過告知 RPC 調用堆疊中的伺服器它們的努力不再必要，從而減少了不必要或註定失敗的工作。為了減少延遲，一些系統使用「對沖請求」(hedged requests) [Dea13] 向主伺服器發送 RPC，然後在一段時間後，向同一服務的其他實例發送相同的請求，以防主伺服器回應緩慢；一旦客戶端從任何伺服器收到回應，它就會向其他伺服器發送訊息以取消現在多餘的請求。這些請求本身可能會遞迴地扇出到許多其他伺服器，因此取消應該在整個堆疊中傳播。

這種方法也可用於避免在初始 RPC 具有長截止日期，但後續堆疊深層之間的關鍵 RPC 收到無法重試成功的錯誤，或具有短截止日期並超時時發生的潛在洩漏。僅使用簡單的截止日期傳播，初始調用會繼續使用伺服器資源，直到它最終超時，儘管註定失敗。將致命錯誤或超時向上傳遞到堆疊並取消調用樹中的其他 RPC，可以防止在整個請求無法完成時進行不必要的工作。

### 雙峰延遲 (Bimodal latency)

假設前述例子中的前端由 10 台伺服器組成，每台伺服器有 100 個工作執行緒。這意味著前端總共有 1,000 個執行緒的容量。在正常操作期間，前端執行 1,000 QPS，請求在 100 毫秒內完成。這意味著前端通常有 100 個工作執行緒被佔用，而配置了 1,000 個工作執行緒（1,000 QPS * 0.1 秒）。

假設一個事件導致 5% 的請求永遠無法完成。這可能是由於某些 Bigtable 行範圍的不可用性，這使得對應於該 Bigtable 鍵空間的請求無法服務。結果，5% 的請求達到了截止日期，而其餘 95% 的請求則花費了通常的 100 毫秒。

在 100 秒的截止日期下，5% 的請求將消耗 5,000 個執行緒（50 QPS * 100 秒），但前端沒有那麼多可用的執行緒。假設沒有其他次要效應，前端將只能處理 19.6% 的請求（1,000 個可用執行緒 / (5,000 + 95) 個執行緒的工作量），導致 80.4% 的錯誤率。

因此，與其只有 5% 的請求收到錯誤（那些因鍵空間不可用而未完成的請求），不如說大多數請求都收到了錯誤。

以下指南可以幫助解決這類問題：

-   檢測這個問題可能非常困難。特別是，當您查看平均延遲時，可能不清楚雙峰延遲是中斷的原因。當您看到延遲增加時，除了平均值外，還應嘗試查看延遲的分佈。
-   如果那些未完成的請求能提早返回錯誤，而不是等待完整的截止日期，就可以避免這個問題。例如，如果一個後端不可用，通常最好立即為該後端返回一個錯誤，而不是消耗資源直到後端可用。如果您的 RPC 層支持**快速失敗 (fail-fast)** 選項，請使用它。
-   截止日期比平均請求延遲長幾個數量級通常是不好的。在前述例子中，一小部分請求最初達到了截止日期，但截止日期比正常平均延遲大三個數量級，導致執行緒耗盡。
-   當使用可能被某些鍵空間耗盡的共享資源時，考慮要麼限制該鍵空間的在途請求，要麼使用其他種類的濫用追蹤。假設您的後端處理來自具有截然不同性能和請求特徵的不同客戶端的請求。您可能會考慮只允許任何一個客戶端佔用您 25% 的執行緒，以便在任何單個客戶端行為不當時提供公平性。

# 慢啟動與冷快取 (Slow Startup and Cold Caching)

進程在剛啟動後回應請求的速度通常比它們在穩態時要慢。這種緩慢可能是由以下一個或兩個原因造成的：

同樣，一些二進位檔案在快取未填充時效率較低。例如，在 Google 的某些服務中，大多數請求都是從快取中服務的，因此未命中快取的請求成本要高得多。在快取溫暖的穩態操作中，只會發生少量快取未命中，但當快取完全為空時，100% 的請求都是昂貴的。其他服務可能會使用快取來將用戶的狀態保存在 RAM 中。這可以通過反向代理和服務前端之間的硬性或軟性**粘性 (stickiness)** 來實現。

如果服務沒有為在**冷快取 (cold cache)** 下處理請求而進行配置，它發生中斷的風險就更大，應該採取措施避免它們。

以下情境可能導致冷快取：

如果快取對服務有顯著影響，111 您可能希望使用以下一種或多種策略：

-   **超額配置服務 (Overprovision the service)**。需要注意的是，**延遲快取 (latency cache)** 與**容量快取 (capacity cache)** 之間的區別：當使用延遲快取時，服務可以用空快取維持其預期負載，但使用容量快取的服務在空快取下無法維持其預期負載。服務所有者應警惕向其服務添加快取，並確保任何新快取要麼是延遲快取，要麼經過足夠好的工程設計以安全地用作容量快取。有時向服務添加快取是為了提高性能，但實際上卻變成了硬依賴。
-   採用通用的級聯故障預防技術。特別是，伺服器在過載或進入降級模式時應拒絕請求，並且應進行測試以查看服務在諸如大規模重啟等事件後的行為。
-   向叢集添加負載時，緩慢增加負載。最初的小請求率會預熱快取；一旦快取溫暖，就可以添加更多流量。確保所有叢集都承擔標稱負載並保持快取溫暖是一個好主意。

## 始終在堆疊中向下走 (Always Go Downward in the Stack)

在莎士比亞服務的例子中，前端與後端通訊，後端又與儲存層通訊。儲存層中出現的問題可能會給與其通訊的伺服器帶來問題，但修復儲存層通常會修復後端和前端層。

然而，假設後端之間相互交叉通訊。例如，當儲存層無法服務請求時，後端可能會相互代理請求以更改哪個後端擁有某個用戶。這種**層內通訊 (intra-layer communication)** 可能有幾個問題：

-   通訊容易受到**分散式死鎖 (distributed deadlock)** 的影響。後端可能使用相同的執行緒池來等待發送到遠端後端的 RPC，而這些遠端後端同時正在接收來自其他遠端後端的請求。假設後端 A 的執行緒池已滿。後端 B 向後端 A 發送一個請求，並在後端 B 中使用一個執行緒，直到後端 A 的執行緒池清空。這種行為可能導致執行緒池飽和度的擴散。
-   如果層內通訊因某種故障或高負載條件而增加（例如，在高負載下更活躍的負載重新平衡），層內通訊可能會在負載增加到足夠大時，迅速從低層內請求模式切換到高層內請求模式。例如，假設一個用戶在一個不同的叢集中有一個主後端和一個預定的熱備份次要後端，可以接管該用戶。主後端由於較低層的錯誤或響應主伺服器上的重負載而將請求代理到次要後端。如果整個系統過載，主到次的代理可能會增加，並由於在主伺服器中解析和等待對次要伺服器的請求的額外成本，給系統增加更多負載。
-   根據跨層通訊的關鍵性，引導系統可能會變得更加複雜。通常最好避免層內通訊——即用戶請求路徑中可能的通訊路徑循環。相反，讓客戶端來進行通訊。例如，如果一個前端與一個後端通訊但猜錯了後端，後端不應該代理到正確的後端。相反，後端應該告訴前端在正確的後端上重試其請求。

# 級聯故障的觸發條件 (Triggering Conditions for Cascading Failures)

當一個服務容易發生級聯故障時，有幾種可能的干擾可以引發多米諾骨牌效應。本節識別了一些觸發級聯故障的因素。

## 進程死亡 (Process Death)

一些伺服器任務可能會死亡，從而減少可用容量。任務可能因為**死亡查詢 (Query of Death)**（其內容觸發進程失敗的 RPC）、叢集問題、斷言失敗或許多其他原因而死亡。一個非常小的事件（例如，幾次崩潰或任務被重新安排到其他機器）可能會使一個處於崩潰邊緣的服務崩潰。

## 進程更新 (Process Updates)

推送新版本的二進位檔案或更新其配置，如果同時影響大量任務，可能會引發級聯故障。為了防止這種情況，要麼在設置服務的更新基礎設施時考慮到必要的容量開銷，要麼在非高峰時段推送。根據請求量和可用容量動態調整進行中的任務更新數量可能是一個可行的方法。

## 新的推出 (New Rollouts)

一個新的二進位檔案、配置更改或對底層基礎設施堆疊的更改，都可能導致請求配置、資源使用和限制、後端或許多其他系統組件的變化，從而觸發級聯故障。

在級聯故障期間，通常明智的做法是檢查最近的更改並考慮恢復它們，特別是如果這些更改影響了容量或改變了請求配置。

您的服務應該實現某種類型的**更改日誌記錄 (change logging)**，這可以幫助快速識別最近的更改。

## 自然增長 (Organic Growth)

在許多情況下，級聯故障並非由特定的服務更改觸發，而是因為使用量的增長沒有伴隨著容量的調整。

## 計劃中的更改、排空或關閉 (Planned Changes, Drains, or Turndowns)

如果您的服務是**多宿主 (multihomed)** 的，由於叢集中的維護或中斷，您的部分容量可能不可用。同樣，服務的一個關鍵依賴項可能被**排空 (drained)**，由於排空依賴關係導致上游服務的容量減少，或者由於必須將請求發送到更遠的叢集而導致延遲增加。

### 請求配置更改

後端服務可能會因為前端服務因負載平衡配置更改、流量混合更改或叢集飽和而轉移其流量，而從不同的叢集接收請求。此外，處理單個負載的平均成本可能因前端程式碼或配置更改而改變。同樣，服務處理的數據可能因現有用戶的使用量增加或不同而有機地改變：例如，對於一個照片儲存服務，每個用戶的照片數量和大小都傾向於隨時間增加。

### 資源限制

一些叢集操作系統允許**資源超額承諾 (resource overcommitment)**。CPU 是一種可互換的資源；通常，一些機器有一些可用的閒置 CPU，這為 CPU 尖峰提供了一點安全網。這種閒置 CPU 的可用性在不同的單元之間以及單元內的機器之間都不同。

依賴這種閒置 CPU 作為您的安全網是危險的。它的可用性完全取決於叢集中其他作業的行為，因此它可能隨時突然消失。例如，如果一個團隊啟動了一個消耗大量 CPU 並在許多機器上調度的 MapReduce，閒置 CPU 的總量可能會突然減少，並為不相關的作業觸發 CPU 饑餓條件。在執行負載測試時，請確保您保持在您承諾的資源限制內。

# 測試級聯故障 (Testing for Cascading Failures)

一個服務將會以何種特定方式失敗，很難從第一原理預測。本節討論了可以檢測服務是否容易發生級聯故障的測試策略。

您應該測試您的服務，以確定它在重負載下的行為，從而獲得信心，它在各種情況下都不會進入級聯故障。

## 測試直到失敗及超越 (Test Until Failure and Beyond)

了解服務在重負載下的行為，也許是避免級聯故障的最重要的第一步。了解您的系統在過載時的行為，有助於確定哪些工程任務對於長期修復最重要；至少，這些知識可能在緊急情況出現時幫助值班工程師啟動除錯過程。

負載測試組件直到它們崩潰。隨著負載增加，一個組件通常會成功處理請求，直到達到一個它無法處理更多請求的點。此時，該組件理想情況下應該開始對額外的負載提供錯誤或降級的結果，但不會顯著降低它成功處理請求的速率。一個極易發生級聯故障的組件在變得過載時會開始崩潰或提供非常高的錯誤率；一個設計得更好的組件則能夠拒絕一些請求並存活下來。

負載測試還揭示了斷裂點在哪裡，這些知識對於容量規劃過程至關重要。它使您能夠測試回歸、為最壞情況的閾值進行配置，以及在利用率與安全邊際之間進行權衡。

由於快取效應，逐漸增加負載可能會產生與立即增加到預期負載水平不同的結果。因此，考慮測試漸進和脈衝負載模式。

您還應該測試並了解組件在被推到遠超其標稱負載後，返回到標稱負載時的行為。這樣的測試可以回答諸如以下問題：

-   如果一個組件在重負載下進入降級模式，它是否能夠在沒有人為干預的情況下退出降級模式？
-   如果幾台伺服器在重負載下崩潰，負載需要下降多少才能使系統穩定下來？

如果您正在對有狀態服務或採用快取的服務進行負載測試，您的負載測試應該追蹤多次互動之間的狀態，並在高負載下檢查正確性，這通常是細微的並發錯誤發生的地方。

請記住，個別組件可能有不同的斷裂點，所以要分別對每個組件進行負載測試。您不會事先知道哪個組件會先撞牆，您想知道您的系統在它撞牆時的行為。

如果您相信您的系統有適當的保護措施以防過載，可以考慮在生產的一小部分進行故障測試，以找出您系統中組件在真實流量下失敗的點。這些限制可能無法通過合成負載測試流量充分反映，因此真實流量測試可能提供比負載測試更真實的結果，但有引起用戶可見痛苦的風險。在真實流量上進行測試時要小心：確保您有額外的可用容量，以防您的自動保護措施不起作用而需要手動故障轉移。您可能會考慮以下一些生產測試：

-   隨著時間的推移，快速或緩慢地減少任務計數，超出預期的流量模式
-   迅速失去一個叢集的容量
-   **黑洞化 (Blackholing)** 各種後端

## 測試流行客戶端 (Test Popular Clients)

了解大型客戶端如何使用您的服務。例如，您想知道客戶端是否：

-   在服務停機時可以排隊工作
-   在錯誤時使用隨機化指數退避
-   容易受到可能產生大量負載的外部觸發器的影響（例如，外部觸發的軟體更新可能會清除離線客戶端的快取）

根據您的服務，您可能無法控制所有與您服務通訊的客戶端程式碼。然而，了解與您服務互動的大型客戶端的行為仍然是一個好主意。

同樣的原則也適用於大型內部客戶端。與最大的客戶端一起進行系統故障演練，看看他們如何反應。詢問內部客戶端他們如何訪問您的服務，以及他們使用什麼機制來處理後端故障。

## 測試非關鍵後端 (Test Noncritical Backends)

測試您的非關鍵後端，並確保它們的不可用性不會干擾您服務的關鍵組件。

例如，假設您的前端有關鍵和非關鍵的後端。通常，一個給定的請求既包括關鍵組件（例如，查詢結果），也包括非關鍵組件（例如，拼寫建議）。您的請求可能會顯著減慢並消耗資源，等待非關鍵後端完成。

除了測試非關鍵後端不可用時的行為外，還要測試如果非關鍵後端從不回應（例如，如果它在黑洞化請求），前端的行為如何。被宣傳為非關鍵的後端在請求有長截止日期時仍然可能給前端帶來問題。當一個非關鍵後端黑洞化時，前端不應該開始拒絕大量請求、耗盡資源或以非常高的延遲提供服務。

# 處理級聯故障的立即步驟 (Immediate Steps to Address Cascading Failures)

一旦您確定您的服務正在經歷級聯故障，您可以使用幾種不同的策略來補救這種情況——當然，級聯故障是使用您的事件管理協議（《管理事件》）的好機會。

## 增加資源 (Increase Resources)

如果您的系統正在以降低的容量運行，並且您有閒置資源，那麼增加任務可能是從中斷中恢復的最權宜之計。然而，如果服務已經進入了某種死亡螺旋，增加更多資源可能不足以恢復。

## 停止健康檢查失敗/死亡 (Stop Health Check Failures/Deaths)

一些叢集調度系統，如 Borg，會檢查作業中任務的健康狀況，並重新啟動不健康的任務。這種做法可能會產生一種故障模式，即健康檢查本身使服務不健康。例如，如果一半的任務因為正在啟動而無法完成任何工作，而另一半很快就會因為過載和健康檢查失敗而被終止，那麼暫時禁用健康檢查可能會讓系統穩定下來，直到所有任務都在運行。

**進程健康檢查**（「這個二進位檔案是否在回應？」）和**服務健康檢查**（「這個二進位檔案現在能否回應這類請求？」）是兩個概念上不同的操作。進程健康檢查與叢集調度器相關，而服務健康檢查與負載平衡器相關。清楚地區分這兩種健康檢查可以幫助避免這種情況。

## 重啟伺服器 (Restart Servers)

如果伺服器因某種原因卡住且沒有進展，重啟它們可能會有所幫助。在以下情況下嘗試重啟伺服器：

-   Java 伺服器處於 GC 死亡螺旋中
-   一些沒有截止日期的在途請求正在消耗資源，導致它們阻塞執行緒，例如
-   伺服器死鎖

在重啟伺服器之前，請確保您已確定級聯故障的來源。確保採取此行動不會僅僅是轉移負載。對此更改進行**金絲雀測試 (Canary this change)**，並緩慢進行。如果中斷實際上是由於冷快取等問題引起的，您的行動可能會放大現有的級聯故障。

## 丟棄流量 (Drop Traffic)

丟棄負載是一把大錘，通常保留給您手上確實有級聯故障且無法通過其他方式解決的情況。例如，如果重負載導致大多數伺服器一健康就崩潰，您可以通過以下方式讓服務重新運行起來：

-   解決初始觸發條件（例如，通過增加容量）。
-   將負載減少到足以停止崩潰的程度。如果整個服務都在崩潰循環中，考慮在這裡採取積極措施，例如，只允許 1% 的流量通過。
-   讓大多數伺服器恢復健康。
-   逐漸增加負載。

這種策略允許快取預熱、建立連接等，然後負載才恢復到正常水平。

顯然，這種策略會對用戶造成很大的可見傷害。您是否能夠（或者您是否應該）無差別地丟棄流量，取決於服務的配置方式。如果您有某種機制可以丟棄不太重要的流量（例如，預取），請首先使用該機制。

重要的是要記住，這種策略使您能夠在根本問題解決後從級聯中斷中恢復。如果引發級聯故障的問題沒有解決（例如，全域容量不足），那麼在所有流量恢復後，級聯故障可能會很快再次觸發。因此，在使用此策略之前，請考慮修復（或至少暫時解決）根本原因或觸發條件。例如，如果服務耗盡了記憶體並且現在處於死亡螺旋中，那麼增加更多記憶體或任務應該是您的第一步。

## 進入降級模式 (Enter Degraded Modes)

通過減少工作量或丟棄不重要的流量來提供降級的結果。這種策略必須被設計到您的服務中，並且只有在您知道哪些流量可以被降級並且您有能力區分各種負載時才能實現。

## 消除批次負載 (Eliminate Batch Load)

一些服務有重要但不關鍵的負載。考慮關閉這些負載源。例如，如果索引更新、數據複製或統計數據收集消耗了服務路徑的資源，考慮在中斷期間關閉這些負載源。

## 消除不良流量 (Eliminate Bad Traffic)

如果某些查詢正在產生重負載或崩潰（例如，死亡查詢），考慮阻止它們或通過其他方式消除它們。

一部關於莎士比亞作品的紀錄片在日本播出，並明確指出我們的莎士比亞服務是進行進一步研究的絕佳地點。廣播後，我們亞洲資料中心的流量激增，超出了服務的容量。這個容量問題因莎士比亞服務在該資料中心同時進行的重大更新而進一步加劇。

幸運的是，有一些保障措施有助於減輕潛在的失敗。**生產準備審查 (Production Readiness Review)** 過程發現了一些團隊已經解決的問題。例如，開發人員在服務中內置了優雅降級。隨著容量變得稀缺，服務不再返回帶有文本的圖片或說明故事發生地的小地圖。而且，根據其目的，超時的 RPC 要麼不被重試（例如，在上述圖片的情況下），要麼以隨機化指數退避的方式重試。儘管有這些保障措施，任務還是一個接一個地失敗，然後被 Borg 重新啟動，這使得工作任務的數量進一步下降。

結果，服務儀表板上的一些圖表變成了驚人的紅色，SRE 被呼叫。作為回應，SRE 們通過增加莎士比亞作業可用的任務數量，臨時增加了亞洲資料中心的容量。通過這樣做，他們能夠恢復亞洲叢集中的莎士比亞服務。

之後，SRE 團隊撰寫了一份事後檢討，詳細說明了事件鏈、哪些地方做得好、哪些地方可以做得更好，以及一些防止這種情況再次發生的行動項目。例如，在服務過載的情況下，GSLB 負載平衡器將把一些流量重定向到鄰近的資料中心。此外，SRE 團隊開啟了**自動擴展 (autoscaling)**，這樣任務數量會隨流量自動增加，他們就不用再擔心這類問題了。

# 結束語 (Closing Remarks)

當系統過載時，需要有所犧牲才能補救這種情況。一旦服務超過其斷裂點，允許一些用戶可見的錯誤或較低品質的結果溜走，比試圖完全服務每個請求要好。了解這些斷裂點在哪裡以及系統在它們之外的行為如何，對於希望避免級聯故障的服務所有者來說至關重要。

如果沒有適當的注意，一些旨在減少背景錯誤或以其他方式改善穩態的系統更改，可能會使服務面臨更大規模中斷的風險。失敗時重試、從不健康的伺服器轉移負載、終止不健康的伺服器、添加快取以提高性能或減少延遲：所有這些都可能是為了改善正常情況而實施的，但都可能增加引發大規模故障的機會。在評估更改時要小心，確保不是用一次中斷換取另一次中斷。

107 見維基百科，「正回饋」，https://en.wikipedia.org/wiki/Positive_feedback 。

108 **看門狗 (watchdog)** 通常實現為一個定期喚醒以查看自上次檢查以來是否已完成工作的執行緒。如果沒有，它就假定伺服器卡住了並終止它。例如，可以定期向伺服器發送已知類型的請求；如果在預期時間內未收到或處理一個請求，這可能表示伺服器、發送請求的系統或中間網路的失敗。

109 由於地理位置的原因，這通常不是一個好的假設；另請參見《作業與數據組織》。

110 一個有啟發性的練習，留給讀者：編寫一個簡單的模擬器，看看後端可以做的有用工作的量如何隨著它被過載的程度和允許的重試次數而變化。

111 有時您會發現，您實際服務容量的很大一部分是作為從快取服務的功能，如果您失去了對該快取的訪問，您實際上將無法服務那麼多查詢。類似的觀察也適用於延遲：快取可以幫助您實現延遲目標（通過在查詢可從快取服務時降低平均回應時間），而沒有該快取您可能無法達到這些目標。