# 數據處理管線 (Data Processing Pipelines)

作者：Dan Dennison
編輯：Tim Harvey

本章重點關注管理具有深度和複雜性的數據處理管線的現實挑戰。它考慮了從非常不頻繁運行的週期性管線到永不停止運行的連續管線的頻率連續體，並討論了可能產生重大運營問題的不連續性。本文提出了一種對領導者-追隨者模型的新看法，作為處理大數據的週期性管線的一種更可靠、擴展性更好的替代方案。

## 管線設計模式的起源

數據處理的經典方法是編寫一個程式，讀取數據，以某種期望的方式進行轉換，並輸出新數據。通常，該程式被排程在像 cron 這樣的週期性排程程式的控制下運行。這種設計模式稱為**數據管線 (data pipeline)**。數據管線可以追溯到協程 [Con63]、DTSS 通訊檔案 [Bul80]、UNIX 管道 [McI86]，以及後來的 ETL 管線，116 但隨著「大數據」的興起，即「數據集如此之大和複雜，以至於傳統的數據處理應用程式已不足以應對」，此類管線獲得了越來越多的關注。117

# 大數據對簡單管線模式的初步影響

對大數據執行週期性或連續性轉換的程式通常被稱為「簡單的單階段管線」。

考慮到大數據固有的規模和處理複雜性，程式通常被組織成一個鏈式系列，一個程式的輸出成為下一個程式的輸入。這種安排可能有各種理由，但通常是為了便於對系統進行推理，而不是通常為了運營效率。以這種方式組織的程式稱為**多階段管線 (multiphase pipelines)**，因為鏈中的每個程式都充當一個離散的數據處理階段。

串聯在一起的程式數量是一種稱為管線**深度 (depth)** 的度量。因此，一個淺層管線可能只有一個程式，其對應的管線深度測量為一，而一個深層管線的管線深度可能在數十或數百個程式。

# 週期性管線模式的挑戰

當有足夠的工作者來處理數據量，並且執行需求在計算能力範圍內時，週期性管線通常是穩定的。此外，當鏈式作業的數量以及作業之間的相對吞吐量保持均勻時，可以避免處理瓶頸等不穩定性。

週期性管線是有用且實用的，我們在 Google 定期運行它們。它們是使用像 MapReduce [Dea04] 和 Flume [Cha10] 等框架編寫的。

然而，SRE 的集體經驗是，週期性管線模型是脆弱的。我們發現，當一個週期性管線首次安裝時，其工作者大小、週期性、分塊技術和其他參數都經過精心調整，性能最初是可靠的。然而，有機的增長和變化不可避免地開始給系統帶來壓力，問題隨之而來。此類問題的例子包括作業超過其運行截止日期、資源耗盡以及導致相應運營負載的**懸掛處理塊 (hanging processing chunks)**。

# 工作分配不均勻引起的麻煩

大數據的關鍵突破是廣泛應用「**易於並行 (embarrassingly parallel)**」[Mol86] 的演算法，將大的工作負載切成足夠小的塊，以適應單個機器。有時，塊之間需要不均勻的資源量，而且最初很少能明顯看出為什麼特定的塊需要不同數量的資源。例如，在按客戶分區的工作負載中，某些客戶的數據塊可能比其他客戶大得多。因為客戶是不可分割的點，所以端到端的運行時間因此受限於最大客戶的運行時間。

當由於叢集中機器的差異或對作業的過度分配而分配資源時，可能會出現「懸掛塊」問題。這個問題是由於某些對流的即時操作（例如對「流式」數據進行排序）的困難而產生的。典型用戶程式碼的模式是等待總計算完成後再進入下一個管線階段，通常是因為可能涉及排序，這需要所有數據才能進行。這可能會顯著延遲管線的完成時間，因為完成被使用的分塊方法所決定的最壞情況性能所阻塞。

如果工程師或叢集監控基礎設施檢測到這個問題，其反應可能會使事情變得更糟。例如，對懸掛塊的「明智」或「預設」反應是立即終止作業，然後允許作業重新啟動，因為阻塞很可能是由不確定性因素造成的。然而，由於管線實現的設計通常不包括**檢查點 (checkpointing)**，所有塊的工作都從頭開始重新啟動，從而浪費了前一個週期投入的時間、CPU 週期和人力。

# 分散式環境中週期性管線的缺點

大數據週期性管線在 Google 被廣泛使用，因此 Google 的叢集管理解決方案為此類管線提供了一種替代的排程機制。這種機制是必要的，因為與連續運行的管線不同，週期性管線通常作為較低優先級的批次作業運行。較低優先級的指定在這種情況下效果很好，因為批次工作對延遲不敏感，不像面向網際網路的 Web 服務那樣。此外，為了通過最大化機器工作負載來控制成本，Borg（Google 的叢集管理系統，[Ver15]）將批次工作分配給可用的機器。這種優先級可能導致啟動延遲下降，因此管線作業可能會經歷無限期的啟動延遲。

通過這種機制調用的作業有許多自然的限制，導致各種不同的行為。例如，排程在面向用戶的 Web 服務作業留下的間隙中的作業，可能會在低延遲資源的可用性、定價和對資源的穩定訪問方面受到影響。執行成本與請求的啟動延遲成反比，與消耗的資源成正比。儘管批次排程在實踐中可能順利進行，但過度使用批次排程器（《使用 Cron 進行分散式定期排程》）會使作業在叢集負載高時面臨被**搶佔 (preemptions)** 的風險（見 [Ver15] 的 2.5 節），因為其他用戶被剝奪了批次資源。鑑於風險權衡，成功運行一個調整良好的週期性管線是在高資源成本和搶佔風險之間的微妙平衡。

對於每天運行的管線來說，長達數小時的延遲可能是可以接受的。然而，隨著排程執行頻率的增加，執行之間的最小時間可以迅速達到最小平均延遲點，從而為週期性管線可以期望達到的延遲設定了一個下限。將作業執行間隔減少到這個有效的下限以下，只會導致不良行為而不是增加進展。具體的故障模式取決於所使用的批次排程策略。例如，由於前一個運行未完成，每個新的運行可能會在叢集排程器上堆積起來。更糟糕的是，當下一次執行排程開始時，當前正在執行且接近完成的運行可能會被終止，以增加執行次數的名義完全停止所有進展。

請注意圖 25-1 中向下傾斜的閒置間隔線與排程延遲相交的位置。在這種情況下，將這個約 20 分鐘作業的執行間隔降低到遠低於 40 分鐘，會導致潛在的重疊執行和不希望的後果。

這個問題的解決方案是確保有足夠的伺服器容量以進行正常操作。然而，在共享的分散式環境中獲取資源受到供需的影響。正如預期的那樣，當資源必須貢獻給一個公共池並共享時，開發團隊往往不願意經歷獲取資源的過程。為了解決這個問題，必須對批次排程資源與生產優先級資源進行區分，以合理化資源獲取成本。

## 週期性管線中的監控問題

對於執行時間足夠長的管線，擁有關於運行時性能指標的即時資訊，可能與了解整體指標同等重要，甚至更重要。這是因為即時數據對於提供運營支持，包括應急響應，非常重要。在實踐中，標準的監控模型涉及在作業執行期間收集指標，並僅在完成時報告指標。如果作業在執行期間失敗，則不提供任何統計數據。

連續管線沒有這些問題，因為它們的任務是不斷運行的，並且它們的遙測通常被設計為即時指標可用。週期性管線不應有固有的監控問題，但我們觀察到一個強烈的關聯。

## 「驚群」問題 ("Thundering Herd" Problems)

除了執行和監控的挑戰之外，還有分散式系統中普遍存在的「**驚群 (thundering herd)**」問題，這在《使用 Cron 進行分散式定期排程》中也有討論。對於一個足夠大的週期性管線，每個週期可能有數千個工作者立即開始工作。如果工作者太多，或者工作者配置不當，或者由錯誤的重試邏輯調用，它們運行的伺服器將不堪重負，底層的共享叢集服務以及任何正在使用的網路基礎設施也將不堪重負。

更糟糕的是，如果沒有實現重試邏輯，當工作在失敗時被丟棄，並且作業不會被重試時，可能會導致正確性問題。如果存在重試邏輯但很天真或實現不佳，失敗時重試可能會加劇問題。

人為干預也可能導致這種情況。管理管線經驗有限的工程師往往會在作業未能在期望的時間內完成時，向其管線添加更多的工作者，從而放大這個問題。

無論「驚群」問題的來源是什麼，對於叢集基礎設施和負責叢集各種服務的 SRE 來說，沒有什麼比一個有錯誤的 10,000 個工作者的管線作業更困難的了。

## 莫列負載模式 (Moiré Load Pattern)

有時，驚群問題在孤立的情況下可能不易發現。一個相關的問題，我們稱之為「**莫列負載模式 (Moiré load pattern)**」，發生在兩個或多個管線同時運行，並且它們的執行序列偶爾重疊，導致它們同時消耗一個共同的共享資源。這個問題即使在連續管線中也可能發生，儘管當負載更均勻地到達時它不太常見。

莫列負載模式在管線使用共享資源的圖中最為明顯。例如，圖 25-2 識別了三個週期性管線的資源使用情況。在圖 25-3 中，這是前一個圖數據的堆疊版本，當總負載接近 1.2M 時，引起值班痛苦的峰值影響發生。

# Google Workflow 介紹

當一個本質上是一次性的批次管線被業務對持續更新結果的需求所壓倒時，管線開發團隊通常會考慮要麼重構原始設計以滿足當前需求，要麼轉向連續管線模型。不幸的是，業務需求通常在最不方便重構管線系統為線上連續處理系統的時候出現。面臨強制擴展問題的更新、更大的客戶通常也希望包含新功能，並期望這些需求遵守不可動搖的截止日期。在預見這一挑戰時，在設計涉及提議的數據管線的系統之初，確定幾個細節是很重要的。務必確定預期的增長軌跡、118 設計修改的需求、預期的額外資源以及業務的預期延遲要求。

面對這些需求，Google 在 2003 年開發了一個名為「Workflow」的系統，該系統使大規模的連續處理成為可能。Workflow 使用**領導者-追隨者 (leader-follower)**（工作者）分散式系統設計模式 [Sha00] 和**系統普遍性 (system prevalence)** 設計模式。119 這種組合使得能夠實現非常大規模的交易性數據管線，並通過**僅一次語義 (exactly-once semantics)** 確保正確性。

## Workflow 作為模型-視圖-控制器模式

由於系統普遍性的工作方式，將 Workflow 視為分散式系統相當於用戶介面開發中已知的**模型-視圖-控制器 (Model-View-Controller, MVC)** 模式可能很有用。120 如圖 25-4 所示，這種設計模式將一個給定的軟體應用程式分成三個相互連接的部分，以將資訊的內部表示與資訊呈現給用戶或從用戶那裡接受的方式分開。121

將此模式應用於 Workflow，模型保存在一個名為「**任務主機 (Task Master)**」的伺服器中。任務主機使用系統普遍性模式將所有作業狀態保存在記憶體中以便快速可用，同時同步地將突變記錄到持久性磁碟。視圖是工作者，它們根據其作為管線子組件的視角，不斷地與主機進行交易性地更新系統狀態。儘管所有管線數據都可以儲存在任務主機中，但通常只有在任務主機中儲存指向工作的指標，而實際的輸入和輸出數據儲存在公共檔案系統或其他儲存中時，才能實現最佳性能。支持這一類比的是，工作者是完全無狀態的，可以隨時被丟棄。可以選擇性地添加一個控制器作為第三個系統組件，以有效地支持影響管線的許多輔助系統活動，例如管線的運行時擴展、快照、工作週期狀態控制、回滾管線狀態，甚至為業務連續性執行全域禁令。圖 25-5 說明了該設計模式。

# Workflow 中的執行階段

我們可以通過將處理細分為保存在任務主機中的任務組，將管線深度增加到任何級別。每個任務組都持有與一個管線階段相對應的工作，該階段可以對某個數據片段執行任意操作。在任何階段執行映射、洗牌、排序、分割、合併或任何其他操作都相對直接。

一個階段通常與某種工作者類型相關聯。可以有多個給定工作者類型的並發實例，並且工作者可以是**自我排程的 (self-scheduled)**，即它們可以尋找不同類型的工作並選擇執行哪種類型。

工作者從前一個階段消耗工作單元並產生輸出單元。輸出可以是一個終點，也可以是其他一些處理階段的輸入。在系統內部，很容易保證所有工作都被執行，或者至少反映在永久狀態中，**僅一次 (exactly once)**。

## Workflow 正確性保證

將管線狀態的每個細節都儲存在任務主機內部是不切實際的，因為任務主機受 RAM 大小的限制。然而，由於主機持有一組指向唯一命名數據的指標，並且每個工作單元都有一個唯一持有的租約，因此存在雙重正確性保證。工作者通過租約獲取工作，並且只能從它們當前擁有有效租約的任務中提交工作。

為了避免孤立的工作者可能繼續在一個工作單元上工作，從而破壞當前工作者工作的​​情況，工作者打開的每個輸出檔案都有一個唯一的名稱。這樣，即使是孤立的工作者也可以獨立於主機繼續寫入，直到它們嘗試提交。在嘗試提交時，它們將無法這樣做，因為另一個工作者持有該工作單元的租約。此外，孤立的工作者無法破壞由有效工作者產生的工作，因為唯一檔名方案確保每個工作者都在寫入一個不同的檔案。這樣，雙重正確性保證得以維持：輸出檔案總是唯一的，並且由於帶有租約的任務，管線狀態總是正確的。

好像雙重正確性保證還不夠，Workflow 還對所有任務進行版本控制。如果任務更新或任務租約更改，每個操作都會產生一個新的唯一任務來替換前一個任務，並為該任務分配一個新的 ID。因為 Workflow 中的所有管線配置都以與工作單元本身相同的形式儲存在任務主機中，所以為了提交工作，工作者必須擁有一個活動的租約，並引用它用來產生其結果的配置的任務 ID 號。如果在工作單元在途時配置發生了更改，則該類型的所有工作者都將無法提交，儘管它們擁有當前的租約。因此，在配置更改後執行的所有工作都與新配置一致，代價是被那些不幸持有舊租約的工作者丟棄的工作。

這些措施提供了三重正確性保證：配置、租約所有權和檔名唯一性。然而，即使這樣也並非適用於所有情況。

例如，如果任務主機的網路位址改變了，並且在同一位址上由一個不同的任務主機替換了它怎麼辦？如果記憶體損壞改變了 IP 位址或端口號，導致另一端出現另一個任務主機怎麼辦？更常見的是，如果有人（錯誤地）通過在一組獨立的任務主機前插入一個負載平衡器來配置他們的任務主機設置怎麼辦？

Workflow 在每個任務的元數據中嵌入一個**伺服器令牌 (server token)**，這是該特定任務主機的唯一標識符，以防止流氓或配置不正確的任務主機損壞管線。客戶端和伺服器在每個操作上都會檢查該令牌，從而避免了一個非常細微的配置錯誤，即所有操作都順利運行，直到發生任務標識符衝突。

總結一下，Workflow 的四個正確性保證是：

-   通過配置任務的工作者輸出創建了工作的前提屏障。
-   所有提交的工作都需要工作者持有當前有效的租約。
-   輸出檔案由工作者唯一命名。
-   客戶端和伺服器通過在每個操作上檢查伺服器令牌來驗證任務主機本身。

在這一點上，您可能會想到，放棄專門的任務主機並使用 Spanner [Cor12] 或其他資料庫會更簡單。然而，Workflow 的特殊之處在於每個任務都是唯一且不可變的。這兩個屬性防止了許多可能因大規模工作分佈而發生的潛在細微問題。

例如，工作者獲得的租約是任務本身的一部分，即使是租約更改也需要一個全新的任務。如果直接使用資料庫，並且其交易日誌像一個「日誌」，那麼每一次讀取都必須是一個長時間運行的交易的一部分。這種配置當然是可能的，但效率極低。

# 確保業務連續性

大數據管線需要在包括光纖切割、天氣事件和級聯電網故障在內的所有類型的故障中繼續處理。這些類型的故障可能會使整個資料中心癱瘓。此外，不採用系統普遍性來獲得關於作業完成的強有力保證的管線，通常會被禁用並進入未定義狀態。這種架構上的差距使得業務連續性策略變得脆弱，並需要昂貴的大規模重複工作來恢復管線和數據。

Workflow 為連續處理管線徹底解決了這個問題。為了獲得全域一致性，任務主機將日誌儲存在 **Spanner** 上，將其用作一個全域可用、全域一致但吞吐量低的檔案系統。為了確定哪個任務主機可以寫入，每個任務主機都使用名為 **Chubby** [Bur06] 的分散式鎖服務來選舉寫入者，並且結果被持久化到 Spanner 中。最後，客戶端使用內部命名服務來查找當前的任務主機。

由於 Spanner 並不是一個高吞吐量的檔案系統，全域分散式的 Workflow 會採用兩個或多個在不同叢集中運行的本地 Workflow，此外還有一個儲存在全域 Workflow 中的參考任務的概念。當工作單元（任務）通過管線被消耗時，標記為「階段 1」的二進位檔案會將等效的參考任務插入到全域 Workflow 中，如圖 25-6 所示。當任務完成時，參考任務會從全域 Workflow 中交易性地移除，如圖 25-6 的「階段 n」所示。如果任務無法從全域 Workflow 中移除，本地 Workflow 將阻塞，直到全域 Workflow 再次可用，從而確保交易的正確性。

為了自動化故障轉移，一個標記為「階段 1」的輔助二進位檔案在每個本地 Workflow 內部運行，如圖 25-6 所示。本地 Workflow 在其他方面保持不變，如圖中的「執行工作」框所示。這個輔助二進位檔案在 MVC 的意義上充當一個「控制器」，負責創建參考任務，以及更新全域 Workflow 內部的一個特殊心跳任務。如果心跳任務在超時期限內未被更新，遠端 Workflow 的輔助二進位檔案將根據參考任務記錄的工作進度接管工作，管線將繼續進行，不受環境可能對工作造成的任何影響。

# 總結與結論

週期性管線很有價值。然而，如果一個數據處理問題是連續的，或者將有機地增長為連續的，就不要使用週期性管線。相反，應使用具有類似 Workflow 特性的技術。

我們發現，具有強有力保證的連續數據處理，如 Workflow 所提供的，在分散式叢集基礎設施上表現良好且擴展性好，通常產生的結果用戶可以信賴，並且對於網站可靠性工程團隊來說是一個穩定可靠的管理和維護系統。

116 維基百科：提取、轉換、加載，https://en.wikipedia.org/wiki/Extract,_transform,_load

117 維基百科：大數據，https://en.wikipedia.org/wiki/Big_data

118 Jeff Dean 關於「從建立大規模分散式系統中學到的軟體工程建議」的講座是一個很好的資源：[Dea07]。

119 維基百科：系統普遍性，https://en.wikipedia.org/wiki/System_Prevalence

120 「模型-視圖-控制器」模式是從 Smalltalk 非常鬆散地借用來的一個分散式系統類比，最初用於描述圖形用戶介面的設計結構 [Fow08]。

121 維基百科：模型-視圖-控制器，https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller