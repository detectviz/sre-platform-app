# 前端負載平衡 (Load Balancing at the Frontend)

作者：Piotr Lewandowski
編輯：Sarah Chavis

我們每秒處理數百萬個請求，您可能已經猜到，我們使用不止一台電腦來應對這一需求。但即使我們擁有一台超級電腦，能夠以某種方式處理所有這些請求（想像一下這種配置所需的網路連接性！），我們仍然不會採用依賴單一故障點的策略；在處理大規模系統時，將所有雞蛋放在一個籃子裡是災難的根源。

本章重點介紹高層次的負載平衡 (load balancing)，即我們如何在資料中心之間平衡用戶流量。下一章將深入探討我們如何在資料中心內部實現負載平衡。

## 算力不是萬能丹 (Power Isn’t the Answer)

為便於討論，讓我們假設我們擁有一台功能強大到令人難以置信的機器和一個永不故障的網路。這樣的配置足以滿足 Google 的需求嗎？不。即使是這樣的配置，仍然會受到我們網路基礎設施相關的物理限制。例如，光速是光纖電纜通訊速度的限制因素，這根據數據必須傳輸的距離，為我們提供數據的速度設定了上限。即使在理想世界中，依賴具有單一故障點的基礎設施也是一個壞主意。

實際上，Google 擁有數千台機器和更多的用戶，其中許多用戶一次會發出多個請求。流量負載平衡 (Traffic load balancing) 是我們決定我們資料中心中眾多機器中的哪一台將處理特定請求的方式。理想情況下，流量以「最佳」方式分佈在多個網路連結、資料中心和機器上。但在這種情況下，「最佳」意味著什麼？實際上沒有單一的答案，因為最佳解決方案在很大程度上取決於多種因素：

- 我們評估問題的層級（全域與本地）
- 我們評估問題的技術層面（硬體與軟體）
- 我們正在處理的流量性質

讓我們從回顧兩種常見的流量情境開始：一個基本的搜尋請求和一個影片上傳請求。用戶希望快速得到他們的查詢結果，因此搜尋請求最重要的變數是延遲 (latency)。另一方面，用戶期望影片上傳需要一段不可忽略的時間，但也希望這樣的請求第一次就能成功，因此影片上傳最重要的變數是吞吐量 (throughput)。這兩種請求的不同需求，在我們如何確定每個請求在全域層級的最佳分佈方面扮演了重要角色：

- 搜尋請求被發送到以**來回通訊時間 (round-trip time, RTT)** 測量的最近的可用資料中心，因為我們希望最小化請求的延遲。
- 影片上傳流被路由到不同的路徑——也許是到一個目前利用率較低的連結，以最大化吞吐量為代價，犧牲延遲。

但在本地層級，在給定的資料中心內部，我們通常假設大樓內的所有機器與用戶的距離相等，並連接到同一個網路。因此，負載的最佳分佈側重於最佳的資源利用率和保護單一伺服器免於過載。

當然，這個例子呈現的是一個極其簡化的畫面。實際上，還有更多的考慮因素影響著最佳的負載分佈：一些請求可能會被導向到一個稍遠的資料中心以保持快取 (caches) 溫暖，或者非互動式流量可能會被路由到一個完全不同的地區以避免網路擁塞。負載平衡，特別是對於大型系統，絕不是直接和靜態的。在 Google，我們透過在多個層級上進行負載平衡來解決這個問題，其中兩個層級將在以下部分描述。為了進行具體的討論，我們將考慮透過 TCP 發送的 HTTP 請求。無狀態服務（如基於 UDP 的 DNS）的負載平衡略有不同，但這裡描述的大多數機制也應適用於無狀態服務。

# 使用 DNS 進行負載平衡 (Load Balancing Using DNS)

在客戶端甚至可以發送 HTTP 請求之前，它通常必須使用 DNS 查閱一個 IP 位址。這為我們引入第一層負載平衡提供了絕佳的機會：**DNS 負載平衡 (DNS load balancing)**。最簡單的解決方案是在 DNS 回應中返回多個 A 或 AAAA 記錄，並讓客戶端任意選擇一個 IP 位址。雖然這個解決方案在概念上很簡單，實現起來也很容易，但它帶來了多個挑戰。

第一個問題是它對客戶端行為的控制非常有限：記錄是隨機選擇的，每個記錄將吸引大致相等的流量。我們能減輕這個問題嗎？理論上，我們可以使用 SRV 記錄來指定記錄的權重和優先級，但 SRV 記錄尚未被 HTTP 採用。

另一個潛在問題源於客戶端通常無法確定最近的位址。我們可以透過為權威域名伺服器 (authoritative nameservers) 使用 **anycast** 位址來緩解這種情況，並利用 DNS 查詢將流向最近位址的事實。在其回應中，伺服器可以返回路由到最近資料中心的位址。進一步的改進是建立一個包含所有網路及其大致物理位置的地圖，並根據該地圖提供 DNS 回應。然而，這個解決方案的代價是需要一個更複雜的 DNS 伺服器實現，並維護一個能夠保持位置映射最新的管道。

當然，由於 DNS 的一個基本特性，這些解決方案都不是輕而易舉的：終端用戶很少直接與權威域名伺服器對話。取而代之的是，一個**遞迴 DNS 伺服器 (recursive DNS server)** 通常位於終端用戶和域名伺服器之間。這個伺服器在用戶和伺服器之間代理查詢，並通常提供一個快取層。這個 DNS 中間人對流量管理有三個非常重要的影響：

- IP 位址的遞迴解析 (Recursive resolution of IP addresses)
- 不確定的回應路徑 (Nondeterministic reply paths)
- 額外的快取複雜性 (Additional caching complications)

IP 位址的遞迴解析是有問題的，因為權威域名伺服器看到的 IP 位址不屬於用戶；相反，它是遞迴解析器的 IP 位址。這是一個嚴重的限制，因為它只允許對解析器和域名伺服器之間最短距離的回應進行優化。一個可能的解決方案是使用 [Con15] 中提出的 **EDNS0** 擴展，它在遞迴解析器發送的 DNS 查詢中包含了客戶端子網的資訊。這樣，權威域名伺服器返回的回應是從用戶的角度來看是最佳的，而不是從解析器的角度。雖然這還不是官方標準，但其明顯的優勢已經讓最大的 DNS 解析器（如 OpenDNS 和 Google 103）支援它。

不僅為給定用戶的請求找到要返回給域名伺服器的最佳 IP 位址很困難，而且該域名伺服器 (nameserver) 可能負責為成千上萬甚至數百萬的用戶提供服務，這些用戶遍布從單一辦公室到整個大陸的不同地區。例如，一個大型的全國性 ISP 可能從一個資料中心為其整個網路運行域名伺服器，但在每個都會區都有網路互連。然後，ISP 的域名伺服器將返回一個最適合其資料中心的 IP 位址的回應，儘管對所有用戶來說都有更好的網路路徑！

最後，遞迴解析器通常會快取回應，並在 DNS 記錄中的**存活時間 (time-to-live, TTL)** 欄位指示的限制內轉發這些回應。最終結果是，估計給定回應的影響是困難的：一個單一的權威回應可能觸及單一用戶或數千名用戶。我們用兩種方式解決這個問題：

- 我們分析流量變化，並不斷更新我們已知的 DNS 解析器列表，其中包含給定解析器背後用戶群的大致規模，這使我們能夠追蹤任何給定解析器的潛在影響。
- 我們估計每個被追蹤解析器背後用戶的地理分佈，以增加我們將這些用戶引導到最佳位置的機會。

估計地理分佈特別棘手，如果用戶群分佈在廣大地區。在這種情況下，我們需要做出權衡，以選擇最佳位置並為大多數用戶優化體驗。

但是在 DNS 負載平衡的背景下，「最佳位置」到底意味著什麼？最明顯的答案是離用戶最近的位置。然而（好像確定用戶位置本身不夠困難似的），還有額外的標準。DNS 負載平衡器需要確保它選擇的資料中心有足夠的容量來服務可能收到其回應的用戶的請求。它還需要知道所選資料中心及其網路連接狀況良好，因為將用戶請求引導到正在經歷電力或網路問題的資料中心並不理想。幸運的是，我們可以將權威 DNS 伺服器與我們的全球控制系統整合，這些系統可以追蹤流量、容量和我們基礎設施的狀態。

DNS 中間人的第三個影響與快取有關。鑑於權威域名伺服器無法刷新解析器的快取，DNS 記錄需要一個相對較低的 TTL。這有效地設定了 DNS 變更可以多快傳播給用戶的下限。104 不幸的是，除了在做負載平衡決策時記住這一點外，我們能做的不多。

儘管有所有這些問題，DNS 仍然是在用戶連接開始之前平衡負載最簡單、最有效的方法。另一方面，應該清楚的是，僅靠 DNS 進行負載平衡是不夠的。請記住，所有提供的 DNS 回應都應符合 RFC 1035 [Moc87] 設定的 512 位元組限制 105。這個限制設定了我們可以在單一 DNS 回應中擠入的位址數量的上限，而這個數字幾乎肯定少於我們的伺服器數量。

要真正解決前端負載平衡的問題，最初的 DNS 負載平衡層級之後，應該跟隨一個利用**虛擬 IP 位址 (virtual IP addresses)** 的層級。

# 在虛擬 IP 位址上進行負載平衡 (Load Balancing at the Virtual IP Address)

**虛擬 IP 位址 (VIPs)** 不會分配給任何特定的網路介面。相反，它們通常在許多設備之間共享。然而，從用戶的角度來看，VIP 仍然是一個單一的、常規的 IP 位址。理論上，這種做法允許我們隱藏實現細節（例如特定 VIP 後面的機器數量）並方便維護，因為我們可以安排升級或向池中添加更多機器而用戶不知情。

實際上，VIP 實現最重要的部分是一種稱為**網路負載平衡器 (network load balancer)** 的設備。平衡器接收封包並將其轉發到 VIP 後面的一台機器。這些後端 (backends) 可以進一步處理請求。

平衡器在決定哪個後端應該接收請求時，有幾種可能的方法。第一種（也許是最直觀的）方法是總是偏好負載最輕的後端。理論上，這種方法應該會帶來最佳的終端用戶體驗，因為請求總是路由到最不繁忙的機器。不幸的是，這種邏輯在有狀態協議 (stateful protocols) 的情況下很快就會失效，這些協議必須在請求的整個持續時間內使用相同的後端。這個要求意味著平衡器必須追蹤所有通過它的連接，以確保所有後續封包都發送到正確的後端。另一種方法是使用封包的某些部分來創建一個連接 ID（可能使用雜湊函數和封包中的一些資訊），並使用該連接 ID 來選擇後端。例如，連接 ID 可以表示為：

`id(packet) mod N`

其中 `id` 是一個將 `packet` 作為輸入並產生連接 ID 的函數，N 是配置的後端數量。

這避免了儲存狀態，並且屬於單一連接的所有封包總是轉發到同一個後端。成功了嗎？還沒有。如果一個後端發生故障並需要從後端列表中移除會發生什麼？突然間 N 變成 N-1，然後 `id(packet) mod N` 就變成了 `id(packet) mod N-1`。幾乎每個封包突然都對應到一個不同的後端！如果後端之間不共享任何狀態，這種重新對應會強制重置幾乎所有現有的連接。這種情況絕對不是最佳的用戶體驗，即使此類事件不常發生。

幸運的是，有一種替代解決方案，它不需要在記憶體中保留每個連接的狀態，但在單一機器故障時也不會強制所有連接重置：**一致性雜湊 (consistent hashing)**。一致性雜湊於 1997 年提出 [Kar97]，描述了一種提供對應演算法的方法，即使在向列表中添加或移除新後端時，該演算法也能保持相對穩定。這種方法在後端池發生變化時，最大限度地減少了對現有連接的干擾。因此，我們通常可以使用簡單的連接追蹤，但在系統壓力過大時（例如，在持續的阻斷服務攻擊期間）則退回到一致性雜湊 (consistent hashing)。

回到更大的問題：網路負載平衡器究竟應該如何將封包轉發到選定的 VIP 後端？一種解決方案是執行**網路位址轉換 (Network Address Translation)**。然而，這需要在追蹤表中保留每個單一連接的條目，這排除了擁有完全無狀態的回退機制的可能性。

另一種解決方案是修改資料鏈路層（OSI 網路模型的第 2 層）上的資訊。通過更改轉發封包的目標 MAC 位址，平衡器可以保持上層所有資訊不變，因此後端接收到原始的來源和目標 IP 位址。然後，後端可以直接向原始發送者發送回覆——這種技術稱為**直接伺服器回應 (Direct Server Response, DSR)**。如果用戶請求很小而回覆很大（例如，大多數 HTTP 請求），DSR 提供了巨大的節省，因為只有一小部分流量需要通過負載平衡器。更好的是，DSR 不需要我們在負載平衡器設備上保留狀態。不幸的是，在大規模部署時，使用第 2 層進行內部負載平衡會帶來嚴重的缺點：所有機器（即所有負載平衡器及其所有後端）必須能夠在資料鏈路層上相互到達。如果網路可以支援這種連接性，並且機器數量沒有過度增長，這不是問題，因為所有機器都需要位於單一的廣播域 (broadcast domain) 中。正如您可能想像的，Google 很久以前就超出了這個解決方案的規模，並且必須找到一種替代方法。

我們目前的 VIP 負載平衡解決方案 [Eis16] 使用**封包裝 (packet encapsulation)**。網路負載平衡器將轉發的封包放入另一個帶有**通用路由封裝 (Generic Routing Encapsulation, GRE)** [Han94] 的 IP 封包中，並使用後端的位址作為目標。接收到封包的後端會剝離外部的 IP+GRE 層，並處理內部的 IP 封包，就好像它被直接傳送到其網路介面一樣。網路負載平衡器和後端不再需要存在於同一個廣播域中；只要兩者之間存在路由，它們甚至可以位於不同的大陸上。

封包裝是一種強大的機制，為我們網路的設計和演進提供了極大的靈活性。不幸的是，封包裝也伴隨著代價：膨脹的封包大小。封包裝會引入開銷（在 IPv4+GRE 的情況下，確切地說是 24 位元組），這可能導致封包超過可用的**最大傳輸單元 (Maximum Transmission Unit, MTU)** 大小並需要進行分片 (fragmentation)。

一旦封包到達資料中心，可以透過在資料中心內部使用更大的 MTU 來避免分片；然而，這種方法需要一個支援大型協定資料單元 (Protocol Data Units) 的網路。與大規模的許多事物一樣，負載平衡表面上聽起來很簡單——儘早且頻繁地進行負載平衡——但困難在於細節，無論是前端負載平衡還是處理到達資料中心的封包。

103 請參閱 https://groups.google.com/forum/#!topic/public-dns-announce/67oxFjSLeUM 。

104 可悲的是，並非所有 DNS 解析器都尊重權威域名伺服器設定的 TTL 值。

105 否則，用戶必須建立 TCP 連接才能獲得 IP 位址列表。