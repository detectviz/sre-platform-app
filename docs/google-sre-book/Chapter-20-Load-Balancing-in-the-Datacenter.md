# 資料中心的負載平衡 (Load Balancing in the Datacenter)

作者：Alejandro Forero Cuervo
編輯：Sarah Chavis

本章重點討論資料中心內部的負載平衡。具體來說，它討論了在給定資料中心內，針對查詢流分配工作的演算法。我們涵蓋了將請求路由到能夠處理它們的個別伺服器的應用層級策略。較低層級的網路原則（例如，交換器、封包路由）和資料中心選擇不在本章的討論範圍內。

假設有一股查詢流到達資料中心——這些查詢可能來自資料中心本身、遠端資料中心，或兩者的混合——其速率不超過資料中心處理它們的資源（或僅在非常短的時間內超過）。同時假設資料中心內有服務，這些查詢是針對這些服務操作的。這些服務被實現為許多同質、可互換的伺服器進程，主要運行在不同的機器上。最小的服務通常至少有三個這樣的進程（使用更少的進程意味著如果失去一台機器，將損失 50% 或更多的容量），而最大的服務可能有多達 10,000 個以上的進程（取決於資料中心的大小）。在典型情況下，服務由 100 到 1,000 個進程組成。我們稱這些進程為**後端任務 (backend tasks)**（或簡稱**後端 (backends)**）。其他任務，稱為**客戶端任務 (client tasks)**，持有到後端任務的連接。對於每個傳入的查詢，客戶端任務必須決定哪個後端任務應該處理該查詢。客戶端使用基於 TCP 和 UDP 組合之上實現的協議與後端進行通訊。

我們應該注意，Google 的資料中心容納了極其多樣化的服務集，這些服務實現了本章討論的策略的不同組合。我們剛才描述的工作範例並不直接適用於任何單一服務。它是一個通用的情境，讓我們能夠討論我們發現對各種服務有用的各種技術。這些技術中的一些可能對特定的使用案例更（或更不）適用，但這些技術是由幾位 Google 工程師在多年的時間裡設計和實現的。

這些技術應用於我們技術堆疊的許多部分。例如，大多數外部 HTTP 請求會到達 GFE (Google 前端)，我們的 HTTP 反向代理系統。GFE 使用這些演算法，以及在《前端負載平衡》中描述的演算法，將請求的負載和元數據路由到運行能夠處理這些資訊的應用程式的各個進程。這是基於一個將各種 URL 模式對應到由不同團隊控制的各個應用程式的配置。為了產生回應的負載（它們將其返回給 GFE，再返回給瀏覽器），這些應用程式通常也反過來使用這些相同的演算法，與它們所依賴的基礎設施或互補服務進行通訊。有時，依賴關係的堆疊會變得相當深，一個傳入的 HTTP 請求可能會觸發一個長的傳遞性依賴請求鏈，到達多個系統，並可能在各個點上具有高的扇出 (fan-out)。

## 理想情況 (The Ideal Case)

在理想情況下，給定服務的負載會完美地分佈在其所有的後端任務上，並且在任何給定的時間點，負載最輕和最重的後端任務消耗的 CPU 完全相同。

我們只能將流量發送到資料中心，直到負載最重的任務達到其容量極限為止；這在圖 20-1 中針對同一時間間隔的兩種情境進行了描述。在那段時間內，跨資料中心的負載平衡演算法必須避免向該資料中心發送任何額外的流量，因為這樣做有使某些任務過載的風險。

如圖 20-2 的左圖所示，大量的容量被浪費了：除了負載最重的任務外，每個任務的閒置容量。

更正式地說，讓 `CPU[i]` 是任務 `i` 在給定時間點消耗的 CPU 速率，並假設任務 0 是負載最重的任務。那麼，在分佈範圍大的情況下，我們浪費的是從任何任務到 `CPU[0]` 的 CPU 差異的總和：也就是說，對所有任務 `i` 的 `(CPU[0] – CPU[i])` 的總和將被浪費。在這種情況下，「浪費」意味著已預留但未使用。

這個例子說明了資料中心內負載平衡實踐不佳如何人為地限制了資源的可用性：您可能為您的服務在給定的資料中心預留了 1,000 個 CPU，但實際上可能無法使用超過，比如說，700 個 CPU。

# 識別不良任務：流量控制與跛腳鴨 (Identifying Bad Tasks: Flow Control and Lame Ducks)

在我們決定哪個後端任務應該接收客戶端請求之前，我們需要識別並避免我們後端池中的不健康任務。

## 處理不健康任務的簡單方法：流量控制 (A Simple Approach to Unhealthy Tasks: Flow Control)

假設我們的客戶端任務追蹤它們在每個到後端任務的連接上發送的活動請求數量。當這個活動請求計數達到配置的限制時，客戶端將該後端視為不健康，並且不再向其發送請求。對於大多數後端來說，100 是一個合理的限制；在平均情況下，請求的完成速度足夠快，以至於在正常操作條件下，給定客戶端的活動請求數量很少會達到這個限制。這種（非常基本的！）流量控制形式也作為一種簡單的負載平衡形式：如果某個後端任務變得過載並且請求開始堆積，客戶端將會避開該後端，工作負載會自然地分佈到其他後端任務中。

不幸的是，這種非常簡單的方法只能保護後端任務免受非常極端的過載形式的影響，而且後端很容易在遠未達到此限制之前就變得過載。反之亦然：在某些情況下，當它們的後端仍有大量備用資源時，客戶端可能會達到此限制。例如，某些後端可能有非常長壽命的請求，這會阻止快速回應。我們見過一些案例，其中這個預設限制產生了反效果，導致所有後端任務都變得無法連線，請求在客戶端被阻塞，直到它們超時並失敗。提高活動請求限制可以避免這種情況，但並不能解決根本問題，即如何知道一個任務是真正不健康還是僅僅是回應緩慢。

## 處理不健康任務的穩健方法：跛腳鴨狀態 (A Robust Approach to Unhealthy Tasks: Lame Duck State)

從客戶端的角度來看，一個給定的後端任務可以處於以下任何一種狀態：

當一個任務進入**跛腳鴨 (lame duck)** 狀態時，它會向其所有活動的客戶端廣播這一事實。但是不活動的客戶端呢？在 Google 的 RPC 實現中，不活動的客戶端（即沒有活動 TCP 連接的客戶端）仍然會發送定期的 UDP 健康檢查。結果是，跛腳鴨資訊會迅速傳播給所有客戶端——通常在 1 或 2 個 RTT 內——無論它們當前的狀態如何。

允許任務存在於準運行的跛腳鴨狀態的主要優點是，它簡化了**乾淨關閉 (clean shutdown)**，從而避免了向所有恰好在正在關閉的後端任務上活動的不幸請求提供錯誤。在不提供任何錯誤的情況下關閉一個有活動請求的後端任務，有助於程式碼推送、維護活動或可能需要重新啟動所有相關任務的機器故障。這樣的關閉將遵循以下一般步驟：

- 工作排程器向後端任務發送一個 `SIGTERM` 信號。
- 後端任務進入跛腳鴨狀態，並要求其客戶端將新請求發送到其他後端任務。這是透過在 `SIGTERM` 處理程序中明確調用的 RPC 實現中的一個 API 呼叫來完成的。
- 在後端任務進入跛腳鴨狀態之前（或在它進入跛腳鴨狀態之後但在客戶端檢測到之前）啟動的任何正在進行的請求都會正常執行。
- 隨著回應流回客戶端，針對該後端的活動請求數量逐漸減少到零。
- 在配置的時間間隔之後，後端任務要么乾淨地退出，要么工作排程器將其終止。該時間間隔應設置得足夠大，以便所有典型的請求都有足夠的時間完成。這個值取決於服務，但一個好的經驗法則是介於 10 秒和 150 秒之間，具體取決於客戶端的複雜性。

這個策略還允許客戶端在執行可能耗時較長的初始化程序時（因此尚未準備好開始服務）與後端任務建立連接。否則，後端任務只能在準備好服務時才開始監聽連接，但這樣做會不必要地延遲連接的協商。一旦後端任務準備好開始服務，它會明確地向客戶端發出信號。

# 使用子集化限制連接池 (Limiting the Connections Pool with Subsetting)

除了健康管理，負載平衡的另一個考慮因素是**子集化 (subsetting)**：限制客戶端任務與之互動的潛在後端任務池。

在我們的 RPC 系統中，每個客戶端都維護一個到其後端的長壽命連接池，用於發送新請求。這些連接通常在客戶端啟動初期建立，並通常保持打開狀態，請求通過它們流動，直到客戶端終止。另一種模型是為每個請求建立和拆除一個連接，但這種模型有顯著的資源和延遲成本。在一個連接長時間保持閒置的極端情況下，我們的 RPC 實現有一個優化，將連接切換到一個廉價的「非活動」模式，例如，其中健康檢查的頻率降低，底層的 TCP 連接被放棄，轉而使用 UDP。

每個連接都需要在兩端消耗一些記憶體和 CPU（由於定期的健康檢查）。雖然這種開銷理論上很小，但當它發生在許多機器上時，很快就會變得非常可觀。子集化避免了單一客戶端連接到非常大量的後端任務，或單一後端任務接收來自非常大量的客戶端任務的連接的情況。在這兩種情況下，您都可能為極小的收益浪費大量的資源。

## 挑選正確的子集 (Picking the Right Subset)

挑選正確的子集歸結為選擇每個客戶端連接到多少個後端任務——即**子集大小 (subset size)**——以及選擇演算法。我們通常使用 20 到 100 個後端任務的子集大小，但一個系統的「正確」子集大小在很大程度上取決於您服務的典型行為。例如，如果出現以下情況，您可能希望使用更大的子集大小：

- 客戶端的數量明顯少於後端的數量。在這種情況下，您希望每個客戶端的後端數量足夠大，以至於不會出現永遠不會收到任何流量的後端任務。
- 客戶端作業中經常出現負載不平衡（即，一個客戶端任務發送的請求多於其他任務）。這種情況在客戶端偶爾發送請求突發的情況下很典型。在這種情況下，客戶端本身會從其他客戶端接收到偶爾有大扇出（例如，「讀取某個用戶所有追隨者的所有資訊」）的請求。因為請求的突發將集中在客戶端分配的子集中，所以您需要一個更大的子集大小，以確保負載在更大的可用後端任務集上均勻分佈。

一旦確定了子集大小，我們需要一個演算法來定義每個客戶端任務將使用的後端任務子集。這看起來可能是一個簡單的任務，但在處理大規模系統時，它很快就會變得複雜，因為在這些系統中，高效的資源配置至關重要，而且系統重啟是必然的。

客戶端的選擇演算法應該均勻地分配後端，以優化資源配置。例如，如果子集化使一個後端過載 10%，那麼整個後端集就需要超額配置 10%。該演算法還應該優雅而穩健地處理重啟和故障，繼續盡可能均勻地加載後端，同時最小化**流失 (churn)**。在這種情況下，「流失」與後端替換選擇有關。例如，當一個後端任務變得不可用時，其客戶端可能需要臨時挑選一個替換後端。當選擇了替換後端時，客戶端必須創建新的 TCP 連接（並可能執行應用層級的協商），這會產生額外的開銷。同樣，當客戶端任務重啟時，它需要重新打開到其所有後端的連接。

該演算法還應該處理客戶端和/或後端數量的調整，連接流失最小，並且事先不知道這些數量。當整個客戶端或後端任務集逐一重啟時（例如，為了推送一個新版本），這個功能尤其重要（且棘手）。在推送後端時，我們希望客戶端能夠透明地繼續服務，連接流失盡可能少。

## 一種子集選擇演算法：隨機子集化 (A Subset Selection Algorithm: Random Subsetting)

一個天真的子集選擇演算法實現可能是讓每個客戶端隨機洗牌一次後端列表，並通過從列表中選擇可解析/健康的後端來填充其子集。洗牌一次然後從列表開頭挑選後端，可以穩健地處理重啟和故障（例如，流失相對較少），因為它明確地將它們排除在考慮之外。然而，我們發現這種策略在大多數實際情境中效果非常差，因為它分佈負載非常不均勻。

在負載平衡的初期工作中，我們實現了隨機子集化，並計算了各種情況下的預期負載。舉個例子，考慮：

- 300 個客戶端
- 300 個後端
- 子集大小為 30%（每個客戶端連接到 90 個後端）

如圖 20-3 所示，負載最輕的後端只有平均負載的 63%（57 個連接，而平均是 90 個連接），而負載最重的則有 121%（109 個連接）。在大多數情況下，30% 的子集大小已經比我們在實踐中想要使用的大了。每次我們運行模擬時，計算出的負載分佈都會改變，而總體模式保持不變。

不幸的是，較小的子集大小會導致更嚴重的不平衡。例如，圖 20-4 描述了如果子集大小減少到 10%（每個客戶端 30 個後端）的結果。在這種情況下，負載最輕的後端接收到平均負載的 50%（15 個連接），而負載最重的接收到 150%（45 個連接）。

我們的結論是，要讓隨機子集化相對均勻地將負載分佈到所有可用任務上，我們需要高達 75% 的子集大小。這麼大的子集根本不切實際；連接到一個任務的客戶端數量的變異數太大，以至於不能將隨機子集化視為一個好的大規模子集選擇策略。

## 一種子集選擇演算法：確定性子集化 (A Subset Selection Algorithm: Deterministic Subsetting)

Google 對隨機子集化局限性的解決方案是**確定性子集化 (deterministic subsetting)**。以下程式碼實現了這個演算法，接下來將詳細描述：

我們將客戶端任務分成「輪次 (rounds)」，其中第 `i` 輪由從任務 `subset_count × i` 開始的 `subset_count` 個連續客戶端任務組成，而 `subset_count` 是子集的數量（即後端任務數除以期望的子集大小）。在每一輪中，每個後端都被精確地分配給一個客戶端（除了可能的最後一輪，它可能不包含足夠的客戶端，所以一些後端可能不會被分配）。

例如，如果我們有 12 個後端任務 `[0, 11]` 和一個期望的子集大小為 3，我們將有每輪包含 4 個客戶端的輪次 (`subset_count = 12/3`)。如果我們有 10 個客戶端，前述演算法可能會產生以下 `shuffled_backends`：

- 第 0 輪: `[0, 6, 3, 5, 1, 7, 11, 9, 2, 4, 8, 10]`
- 第 1 輪: `[8, 11, 4, 0, 5, 6, 10, 3, 2, 7, 9, 1]`
- 第 2 輪: `[8, 3, 7, 2, 1, 4, 9, 10, 6, 5, 0, 11]`

需要注意的關鍵點是，每一輪只將整個列表中的每個後端分配給一個客戶端（除了最後一輪，我們可能會用完客戶端）。在這個例子中，每個後端被分配給恰好兩個或三個客戶端。

列表應該被洗牌；否則，客戶端會被分配一組連續的後端任務，這些任務可能會全部暫時不可用（例如，因為後端作業正在從第一個任務到最後一個任務按順序逐步更新）。不同的輪次使用不同的種子進行洗牌。如果它們不這樣做，當一個後端失敗時，它接收的負載只會分佈在其子集中的其餘後端之間。如果子集中的其他後端也失敗，效果會疊加，情況會迅速惡化：如果子集中的 N 個後端宕機，它們相應的負載會分佈在剩餘的 `(subset_size - N)` 個後端上。一個好得多的方法是通過為每一輪使用不同的洗牌，將這個負載分佈到所有剩餘的後端上。

當我們為每一輪使用不同的洗牌時，同一輪中的客戶端將以相同的洗牌列表開始，但跨輪次的客戶端將有不同的洗牌列表。從這裡，演算法根據後端的洗牌列表和期望的子集大小來建立子集定義。例如：

- `Subset[0] = shuffled_backends[0]` 到 `shuffled_backends[2]`
- `Subset[1] = shuffled_backends[3]` 到 `shuffled_backends[5]`
- `Subset[2] = shuffled_backends[6]` 到 `shuffled_backends[8]`
- `Subset[3] = shuffled_backends[9]` 到 `shuffled_backends[11]`

其中 `shuffled_backend` 是每個客戶端創建的洗牌列表。要將一個子集分配給一個客戶端任務，我們只需取與其在輪次中位置對應的子集（例如，對於有四個子集的 `client[i]`，使用 `(i % 4)`）：

- `client[0]`、`client[4]`、`client[8]` 將使用 `subset[0]`
- `client[1]`、`client[5]`、`client[9]` 將使用 `subset[1]`
- `client[2]`、`client[6]`、`client[10]` 將使用 `subset[2]`
- `client[3]`、`client[7]`、`client[11]` 將使用 `subset[3]`

因為跨輪次的客戶端將使用不同的 `shuffled_backends` 值（因此也使用不同的 `subset`），並且輪次內的客戶端使用不同的子集，所以連接負載是均勻分佈的。在後端總數不能被期望的子集大小整除的情況下，我們允許少數子集比其他子集稍大一些，但在大多數情況下，分配給一個後端的客戶端數量最多相差 1。

如圖 20-5 所示，前述 300 個客戶端每個連接到 300 個後端中的 10 個的例子，其分佈產生了非常好的結果：每個後端接收到完全相同數量的連接。

# 負載平衡策略 (Load Balancing Policies)

現在我們已經為給定的客戶端任務如何維護一組已知健康的連接奠定了基礎，讓我們來看看**負載平衡策略 (load balancing policies)**。這些是客戶端任務用來選擇其子集中的哪個後端任務接收客戶端請求的機制。負載平衡策略中的許多複雜性源於決策過程的分散性，在這種過程中，客戶端需要即時（並且僅有部分和/或過時的後端狀態資訊）決定應該為每個請求使用哪個後端。

負載平衡策略可以非常簡單，不考慮任何關於後端狀態的資訊（例如，**循環輪詢 (Round Robin)**），也可以根據更多關於後端的資訊來行動（例如，**最少負載循環輪詢 (Least-Loaded Round Robin)** 或 **加權循環輪詢 (Weighted Round Robin)**）。

## 簡單循環輪詢 (Simple Round Robin)

一種非常簡單的負載平衡方法是讓每個客戶端以循環輪詢的方式向其子集中可以成功連接且不處於跛腳鴨狀態的每個後端任務發送請求。多年來，這是我們最常用的方法，並且仍有許多服務在使用它。

不幸的是，雖然循環輪詢具有非常簡單且性能明顯優於僅隨機選擇後端任務的優點，但該策略的結果可能非常差。雖然實際數字取決於許多因素，例如變動的查詢成本和機器多樣性，但我們發現循環輪詢可能導致從負載最輕到最重的任務的 CPU 消耗相差高達 2 倍。這樣的差距極其浪費，並且由多種原因造成，包括：

- 小的子集化 (Small subsetting)
- 變動的查詢成本 (Varying query costs)
- 機器多樣性 (Machine diversity)
- 不可預測的性能因素 (Unpredictable performance factors)

### 小的子集化 (Small subsetting)

循環輪詢分佈負載不佳的最簡單原因之一是，並非所有客戶端都以相同的速率發出請求。當極不相同的進程共享相同的後端時，客戶端之間的請求速率差異尤其可能出現。在這種情況下，特別是如果您使用的是相對較小的子集大小，產生最多流量的客戶端子集中的後端自然會傾向於負載更重。

### 變動的查詢成本 (Varying query costs)

許多服務處理需要極不相同資源量的請求。在實踐中，我們發現 Google 中許多服務的語義是，最昂貴的請求消耗的 CPU 比最便宜的請求多 1000 倍（或更多）。當無法預先預測查詢成本時，使用循環輪詢進行負載平衡更加困難。例如，一個諸如「返回用戶 XYZ 在過去一天內收到的所有電子郵件」的查詢可能非常便宜（如果用戶在這一天內收到的郵件很少），也可能極其昂貴。

在一個具有巨大潛在查詢成本差異的系統中進行負載平衡是非常有問題的。可能有必要調整服務介面，以功能性地限制每個請求完成的工作量。例如，在前面描述的電子郵件查詢的情況下，您可以引入一個分頁介面，並將請求的語義更改為「返回用戶 XYZ 在過去一天內收到的最近 100 封（或更少）電子郵件」。不幸的是，引入這樣的語義更改通常很困難。這不僅需要更改所有客戶端程式碼，還需要額外的一致性考慮。例如，當客戶端逐頁獲取電子郵件時，用戶可能正在接收新郵件或刪除郵件。對於這種使用案例，一個天真地遍歷結果並連接回應的客戶端（而不是基於固定數據視圖進行分頁）可能會產生不一致的視圖，重複某些訊息和/或跳過其他訊息。

為了保持介面（及其實現）的簡單性，服務通常被定義為允許最昂貴的請求消耗比最便宜的請求多 100、1,000 甚至 10,000 倍的資源。然而，每個請求的資源需求不同，自然意味著一些後端任務會不幸地偶爾接收到比其他任務更昂貴的請求。這種情況對負載平衡的影響程度取決於最昂貴的請求有多昂貴。例如，對於我們的一個 Java 後端，查詢平均消耗約 15 毫秒的 CPU，但一些查詢可以輕易地需要長達 10 秒。這個後端中的每個任務都預留了多個 CPU 核心，這通過允許一些計算並行進行來減少延遲。但是儘管有這些預留的核心，當一個後端接收到這些大型查詢之一時，其負載會在幾秒鐘內顯著增加。一個行為不佳的任務可能會耗盡記憶體，甚至完全停止回應（例如，由於記憶體顛簸），但即使在正常情況下（即後端有足夠的資源，並且其負載在大型查詢完成後恢復正常），其他請求的延遲也會因為與昂貴請求的資源競爭而受到影響。

### 機器多樣性 (Machine diversity)

簡單循環輪詢的另一個挑戰是，同一資料中心中的所有機器不一定都相同。一個給定的資料中心可能有不同性能的 CPU 的機器，因此，相同的請求對不同的機器可能代表著顯著不同的工作量。

多年來，在 Google，處理機器多樣性而不需要嚴格的同質性是一個挑戰。理論上，處理機群中異構資源容量的解決方案很簡單：根據處理器/機器類型調整 CPU 預留。然而，在實踐中，推出這個解決方案需要大量的努力，因為它要求我們的作業排程器根據抽樣服務的平均機器性能來計算資源等價性。例如，機器 X（一台「慢」機器）中的 2 個 CPU 單元相當於機器 Y（一台「快」機器）中的 0.8 個 CPU 單元。有了這些資訊，作業排程器就需要根據等價因子和進程被排程到的機器類型來調整進程的 CPU 預留。為了減輕這種複雜性，我們創建了一個名為「GCU」（Google 計算單元）的 CPU 速率虛擬單元。GCU 成為了模擬 CPU 速率的標準，並被用來維護一個從我們資料中心中每個 CPU 架構到其相應 GCU 的映射，基於其性能。

### 不可預測的性能因素 (Unpredictable performance factors)

也許對簡單循環輪詢來說，最大的複雜因素是機器，或者更準確地說，後端任務的性能可能會因幾個無法靜態考慮的不可預測方面而大相徑庭。

導致性能的許多不可預測因素中的兩個包括：

如果您的負載平衡策略無法適應不可預見的性能限制，那麼在規模化工作時，您將不可避免地得到一個次優的負載分佈。

## 最少負載循環輪詢 (Least-Loaded Round Robin)

簡單循環輪詢的另一種方法是讓每個客戶端任務追蹤它對其子集中每個後端任務的活動請求數量，並在**具有最少活動請求數量的任務集**中使用循環輪詢。

例如，假設一個客戶端使用後端任務 `t0` 到 `t9` 的子集，並且目前對每個後端有以下數量的活動請求：

`2 1 0 0 1 0 2 0 0 1`

對於一個新請求，客戶端會將潛在後端任務的列表過濾為只有那些連接數最少的任務（`t2`, `t3`, `t5`, `t7`, 和 `t8`），並從該列表中選擇一個後端。讓我們假設它選擇了 `t2`。客戶端的連接狀態表現在看起來會是這樣：

`2 1 1 0 1 0 2 0 0 1`

假設當前的請求都沒有完成，在下一個請求時，後端候選池變成了 `t3`, `t5`, `t7`, 和 `t8`。

讓我們快進到我們發出了四個新請求。仍然假設在此期間沒有請求完成，連接狀態表看起來會是這樣：

`2 1 1 1 1 1 2 1 1 1`

此時，後端候選集是除了 `t0` 和 `t6` 之外的所有任務。然而，如果針對任務 `t4` 的請求完成，其當前狀態變為「0 個活動請求」，一個新請求將被分配給 `t4`。

這個實現實際上使用了循環輪詢，但它是應用在具有最少活動請求的任務集上。如果沒有這樣的過濾，該策略可能無法足夠好地分佈請求，以避免一部分可用後端任務未被使用的情況。最少負載策略背後的想法是，負載較重的任務往往比有備用容量的任務具有更高的延遲，而這個策略會自然地將負載從這些負載較重的任務上移開。

儘管如此，我們（以慘痛的方式！）了解到最少負載循環輪詢方法的一個非常危險的陷阱：如果一個任務嚴重不健康，它可能會開始提供 100% 的錯誤。根據這些錯誤的性質，它們的延遲可能非常低；僅僅返回一個「我不健康！」的錯誤通常比實際處理一個請求要快得多。結果，客戶端可能會開始向不健康的任務發送大量的流量，錯誤地認為該任務是可用的，而不是在快速地讓它們失敗！我們說不健康的任務現在正在**吸收流量 (sinkholing traffic)**。幸運的是，這個陷阱可以相對容易地解決，方法是修改策略，將最近的錯誤計為活動請求。這樣，如果一個後端任務變得不健康，負載平衡策略就會開始將負載從它身上轉移，就像它會從一個過載的任務身上轉移負載一樣。

最少負載循環輪詢有兩個重要的限制：

在實踐中，我們發現使用最少負載循環輪詢的大型服務，其負載最重的後端任務使用的 CPU 是最輕的兩倍，表現與循環輪詢差不多差。

## 加權循環輪詢 (Weighted Round Robin)

**加權循環輪詢 (Weighted Round Robin)** 是一個重要的負載平衡策略，它通過將後端提供的資訊納入決策過程，改進了簡單和最少負載循環輪詢。

加權循環輪詢的原理相當簡單：每個客戶端任務為其子集中的每個後端保留一個「能力」分數。請求以循環輪詢的方式分佈，但客戶端按比例加權對後端的請求分佈。在每個回應中（包括對健康檢查的回應），後端除了包含利用率（通常是 CPU 使用率）外，還包含當前觀察到的每秒查詢和錯誤率。客戶端定期調整能力分數，以根據它們當前處理的成功請求數量及其利用率成本來挑選後端任務；失敗的請求會導致一個影響未來決策的懲罰。

在實踐中，加權循環輪詢效果非常好，並顯著減少了利用率最高和最低任務之間的差異。圖 20-6 顯示了在其客戶端從最少負載切換到加權循環輪詢時，一個隨機後端任務子集的 CPU 速率。從負載最輕到最重的任務的差距急劇縮小。