# 第三部分：實踐

簡單來說，SRE 運行**服務 (services)**——一組為用戶（可能是內部或外部用戶）運營的相關系統，並最終對這些服務的健康狀況負責。成功運營一個服務需要進行廣泛的活動：開發監控系統、規劃容量、應對事件、確保解決中斷的根本原因等等。本節討論 SRE 日常活動的理論與實踐：建構和運營大型分散式計算系統。

我們可以像亞伯拉罕·馬斯洛對人類需求的分類 [Mas43] 一樣來描述一個服務的健康狀況——從一個系統作為服務運作所需的最基本要求，到允許自我實現並主動控制服務方向，而不是被動地救火的更高層次的功能。這種理解對我們在 Google 評估服務的方式至關重要，以至於直到我們的一些 Google SRE，包括我們的前同事 Mikey Dickerson，41 在 2013 年末和 2014 年初暫時加入美國政府截然不同的文化，幫助推出 healthcare.gov 時，才明確地發展起來：他們需要一種方法來解釋如何提高系統的可靠性。

我們將使用這個層次結構，如圖 3-1 所示，來審視構成一個可靠服務的元素，從最基本到最先進。

**監控 (Monitoring)**

如果沒有監控，您就無法判斷服務是否正常工作；如果沒有經過深思熟慮設計的監控基礎設施，您就是在盲目飛行。也許每個試圖使用網站的人都會遇到錯誤，也許不會——但您希望在用戶注意到問題之前就意識到問題。我們在「來自時間序列資料的實用警報」中討論了工具和理念。

**事件回應 (Incident Response)**

SRE 並非僅僅為了值班而值班：相反，值班支援是我們用來實現更大使命並與分散式計算系統實際工作（和失敗！）方式保持聯繫的工具。如果我們能找到一種方法讓我們自己擺脫攜帶呼叫器的負擔，我們會的。在「值班 (Being On-Call)」中，我們解釋了我們如何平衡值班職責與我們的其他責任。

一旦您意識到存在問題，您如何讓它消失？這並不一定意味著一勞永逸地解決它——也許您可以通過降低系統的精度或暫時關閉某些功能來止血，使其**優雅降級 (gracefully degrade)**，或者您可以將流量引導到另一個正常工作的服務實例。您選擇實施的解決方案的細節必然是特定於您的服務和您的組織的。然而，有效地應對事件是適用於所有團隊的事情。

找出問題所在是第一步；我們在「高效的故障排除」中提供了一種結構化的方法。

在事件期間，人們常常很容易屈服於腎上腺素，開始臨時應對。我們在「緊急應變」中建議不要屈服於這種誘惑，並在「事件管理」中建議，有效地管理事件應該可以減少其影響並限制由中斷引起的焦慮。

**事後檢討與根本原因分析 (Postmortem and Root-Cause Analysis)**

我們的目標是只對我們服務呈現的新的、令人興奮的問題發出警報並手動解決；一遍又一遍地「修復」同一個問題是極其乏味的。事實上，這種心態是 SRE 理念與一些更傳統的以運營為中心的環境之間的關鍵區別之一。這個主題在兩章中進行了探討。

建立一個**無指責的事後檢討文化 (blameless postmortem culture)** 是理解哪裡出了問題（以及哪裡做對了！）的第一步，如「事後檢討文化：從失敗中學習」中所述。

與該討論相關，在「追蹤服務中斷」中，我們簡要描述了一個內部工具，即中斷跟踪器，它允許 SRE 團隊跟踪最近的生產事件、其原因以及為應對它們而採取的行動。

**測試 (Testing)**

一旦我們了解了哪些地方容易出錯，我們的下一步就是試圖預防它，因為一盎司的預防勝過一磅的治療。測試套件提供了一些保證，確保我們的軟體在發布到生產環境之前不會出現某些類別的錯誤；我們在「可靠性測試」中討論了如何最好地使用這些套件。

**容量規劃 (Capacity Planning)**

在「SRE 中的軟體工程」中，我們提供了一個 SRE 軟體工程的案例研究，介紹了 Auxon，一個用於自動化容量規劃的工具。

自然而然地，在容量規劃之後，**負載平衡 (load balancing)** 確保我們正確地使用了我們建立的容量。我們在「前端負載平衡」中討論了對我們服務的請求如何被發送到資料中心。然後我們在「資料中心的負載平衡」和「處理過載」中繼續討論，這兩者對於確保服務可靠性都至關重要。

最後，在「解決連鎖故障」中，我們為解決連鎖故障提供了建議，無論是在系統設計中，還是當您的服務陷入連鎖故障時。

**開發 (Development)**

Google 網站可靠性工程方法的一個關鍵方面是，我們在組織內部進行了大量的大規模系統設計和軟體工程工作。

在「管理關鍵狀態：為可靠性而生的分散式共識」中，我們解釋了**分散式共識 (distributed consensus)**，它（以 Paxos 的形式）是許多 Google 分散式系統的核心，包括我們全球分佈的 Cron 系統。在「使用 Cron 進行分散式定期排程」中，我們概述了一個擴展到整個資料中心及更廣範圍的系統，這並非易事。

「資料處理管線」討論了資料處理管線可以採取的各種形式：從定期運行的一次性 MapReduce 工作到近乎即時運行的系統。不同的架構可能導致令人驚訝和違反直覺的挑戰。

確保您儲存的資料在您想要讀取時仍然存在是**資料完整性 (data integrity)** 的核心；在「資料完整性：所讀即所寫」中，我們解釋瞭如何保護資料安全。

**產品 (Product)**

最後，在可靠性金字塔上爬升之後，我們發現自己處於擁有一個可行產品的點。在「可靠的大規模產品發布」中，我們撰寫了關於 Google 如何進行可靠的大規模產品發布，以試圖從第一天起就為用戶提供最佳體驗。

**來自 Google SRE 的進階閱讀**

如前所述，測試是微妙的，其不當執行可能對整體穩定性產生重大影響。在一篇 ACM 文章 [Kri12] 中，我們解釋了 Google 如何進行全公司範圍的彈性測試，以確保我們有能力應對意外情況，無論是殭屍末日還是其他災難。

雖然容量規劃通常被認為是一門玄學，充滿了神秘的電子表格來預測未來，但它仍然至關重要，正如 [Hix15a] 所示，您實際上並不需要水晶球就能把它做好。

最後，[War14] 中詳細介紹了一種有趣且新穎的公司網路安全方法，該計畫旨在用設備和用戶憑證取代特權內部網路。由基礎設施層級的 SRE 推動，這絕對是您在創建下一個網路時應該牢記的方法。

41 Mikey 於 2014 年夏天離開 Google，成為美國數位服務署 (https://www.usds.gov/) 的第一任署長，該機構旨在（部分）將 SRE 的原則和實踐引入美國政府的 IT 系統。