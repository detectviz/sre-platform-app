# 從 SRE 的角度看 Google 的生產環境

作者：JC van Winkel 編輯：Betsy Beyer

Google 的資料中心與大多數傳統的資料中心和小型伺服器機房非常不同。這些差異帶來了額外的問題和機會。本章將討論 Google 資料中心的特點所帶來的挑戰和機會，並介紹本書中將使用的術語。

## 硬體

Google 的大部分計算資源都位於 Google 設計的資料中心，這些資料中心擁有專有的配電、冷卻、網路和計算硬體（請參閱 [Bar13]）。與「標準」的託管資料中心不同，Google 設計的資料中心中的計算硬體在各方面都是相同的。9 為了消除伺服器硬體和伺服器軟體之間的混淆，我們在本書中統一使用以下術語：

機器 (Machines) 可以運行任何伺服器，所以我們不會將特定的機器專用於特定的伺服器程式。例如，沒有特定的機器運行我們的郵件伺服器。相反，資源分配由我們的叢集作業系統 Borg 處理。

我們意識到「伺服器 (server)」這個詞的用法並不尋常。這個詞的常見用法將「接受網路連線的二進位檔案」與「機器 (machine)」混為一談，但在討論 Google 的計算時，區分這兩者非常重要。一旦您習慣了我們對「伺服器」的用法，就會更清楚為什麼使用這種專門的術語是有意義的，不僅在 Google 內部，在本書的其餘部分也是如此。

圖 2-1 說明了 Google 資料中心的拓撲結構：

數十台機器被放置在一個機櫃 (rack) 中。機櫃排列成行 (row)。一行或多行組成一個叢集 (cluster)。通常一個資料中心建築內會有多個叢集。多個相鄰的資料中心建築組成一個園區 (campus)。

- 數十台機器被放置在一個**機櫃 (rack)** 中。
- 機櫃排列成**行 (row)**。
- 一行或多行組成一個**叢集 (cluster)**。
- 通常一個資料中心建築內會有多個叢集。
- 多個相鄰的資料中心建築組成一個**園區 (campus)**。

在給定的資料中心內，機器之間需要能夠相互通訊，因此我們創建了一個擁有數萬個埠的超高速虛擬交換器。我們通過將數百個 Google 自製的交換器連接成一個名為 Jupiter [Sin15] 的 Clos 網路結構 [Clos53] 來實現這一點。在其最大配置下，Jupiter 支援伺服器之間 1.3 Pbps 的對分頻寬。

資料中心之間通過我們遍布全球的骨幹網路 B4 [Jai13] 連接。B4 是一種軟體定義網路 (software-defined networking) 架構（並使用 OpenFlow 開放標準通訊協定）。它為數量不多的站點提供巨大的頻寬，並利用彈性頻寬分配來最大化平均頻寬 [Kum15]。

# 「組織」硬體的系統軟體

我們的硬體必須由能夠處理大規模的軟體來控制和管理。硬體故障是我們用軟體來管理的一個顯著問題。考慮到一個叢集中的大量硬體組件，硬體故障發生的頻率相當高。在一個典型的年份裡，單一叢集中會有數千台機器故障，數千個硬碟損壞；如果將這個數字乘以我們在全球運營的叢集數量，這些數字會變得有些驚人。因此，我們希望為用戶抽象化這些問題，同樣地，運行我們服務的團隊也不希望被硬體故障所困擾。每個資料中心園區都有專門的團隊負責維護硬體和資料中心基礎設施。

## 管理機器

如圖 2-2 所示，**Borg** 是一個分散式叢集作業系統 [Ver15]，類似於 Apache Mesos。10 Borg 在叢集層級管理其**工作 (jobs)**。

Borg 負責運行用戶的**工作 (jobs)**，這些工作可以是不斷運行的伺服器，也可以是像 MapReduce [Dea04] 這樣的批次處理程序。工作可以由一個以上（有時是數千個）相同的**任務 (tasks)** 組成，這既是出於可靠性的考慮，也是因為單一程序通常無法處理所有的叢集流量。當 Borg 啟動一個工作時，它會為這些任務尋找機器，並告知這些機器啟動伺服器程式。然後，Borg 會持續監控這些任務。如果一個任務發生故障，它會被終止並重新啟動，可能會在不同的機器上。

由於任務是在機器之間流動分配的，我們不能簡單地依靠 IP 位址和埠號來引用任務。我們通過增加一個間接層來解決這個問題：在啟動工作時，Borg 使用 **Borg 命名服務 (Borg Naming Service, BNS)** 為每個任務分配一個名稱和索引號。其他程序通過 BNS 名稱連接到 Borg 任務，而不是使用 IP 位址和埠號，BNS 會將名稱轉換為 IP 位址和埠號。例如，BNS 路徑可能是一個字串，如 `/bns/<叢集>/<用戶>/<工作名稱>/<任務編號>`，它會解析為 `<IP 位址>:<埠號>`。

Borg 也負責將資源分配給工作。每個工作都需要指定其所需的資源（例如，3 個 CPU 核心，2 GiB 的 RAM）。利用所有工作的需求列表，Borg 可以以最佳方式將任務打包到機器上，同時也會考慮到故障域（例如：Borg 不會將一個工作的所有任務都運行在同一個機櫃上，因為這樣做意味著機櫃頂端交換器 (top of rack switch) 成為該工作的單點故障）。

如果一個任務試圖使用比其請求更多的資源，Borg 會終止該任務並重新啟動它（因為一個緩慢崩潰循環的任務通常比一個完全沒有重新啟動的任務要好）。

## 儲存

任務可以使用機器上的本地磁碟作為臨時空間，但我們有多種叢集儲存選項用於永久儲存（甚至臨時空間最終也將轉移到叢集儲存模型）。這些選項可與 Lustre 和 Hadoop 分散式檔案系統 (HDFS) 相媲美，兩者都是開源的叢集檔案系統。

儲存層負責為用戶提供簡單可靠的方式來存取叢集中可用的儲存。如圖 2-3 所示，儲存有多個層次：

- 最底層稱為 **D**（代表**磁碟 (disk)**，儘管 D 同時使用傳統硬碟和快閃記憶體）。D 是一個在叢集中幾乎所有機器上運行的檔案伺服器。然而，想要存取資料的用戶不希望記住哪台機器儲存了他們的資料，這就是下一層發揮作用的地方。
- 在 D 之上的一層稱為 **Colossus**，它創建了一個叢集範圍的檔案系統，提供常規的檔案系統語意，以及複製和加密功能。Colossus 是 GFS（Google 檔案系統 [Ghe03]）的後繼者。
- 在 Colossus 之上建立了幾個類似資料庫的服務：**Bigtable** [Cha06] 是一個 NoSQL 資料庫系統，可以處理 PB 等級大小的資料庫。Bigtable 是一個稀疏、分散式、持久的多維排序對應，由行鍵、列鍵和時間戳索引；對應中的每個值都是一個未經解釋的位元組陣列。Bigtable 支援最終一致性、跨資料中心的複製。**Spanner** [Cor12] 為需要全球範圍內真正一致性的用戶提供類似 SQL 的介面。還有其他幾個資料庫系統，如 **Blobstore**。這些選項中的每一個都有其自身的權衡取捨（請參閱「資料完整性：所讀即所寫」）。

在 Colossus 之上建立了幾個類似資料庫的服務：

- **Bigtable** [Cha06] 是一個 NoSQL 資料庫系統，可以處理 PB 等級大小的資料庫。Bigtable 是一個稀疏、分散式、持久的多維排序對應，由行鍵、列鍵和時間戳索引；對應中的每個值都是一個未經解釋的位元組陣列。Bigtable 支援最終一致性、跨資料中心的複製。
- **Spanner** [Cor12] 為需要全球範圍內真正一致性的用戶提供類似 SQL 的介面。
- 還有其他幾個資料庫系統，如 **Blobstore**。這些選項中的每一個都有其自身的權衡取捨（請參閱「資料完整性：所讀即所寫」）。

## 網路

Google 的網路硬體以多種方式進行控制。如前所述，我們使用基於 OpenFlow 的軟體定義網路。我們不使用「智慧型」路由硬體，而是依賴較便宜的「傻瓜型」交換組件，並結合一個中央（備份的）控制器來預先計算整個網路的最佳路徑。因此，我們能夠將計算密集型的路由決策從路由器上移開，並使用簡單的交換硬體。

網路頻寬需要明智地分配。正如 Borg 限制任務可以使用的計算資源一樣，**頻寬強制執行器 (Bandwidth Enforcer, BwE)** 管理可用的頻寬，以最大化平均可用頻寬。優化頻寬不僅僅是為了成本：集中式流量工程已被證明可以解決許多傳統上極難通過分散式路由和流量工程組合解決的問題 [Kum15]。

有些服務的工作運行在多個叢集中，這些叢集分佈在世界各地。為了最小化全球分散式服務的延遲，我們希望將用戶引導到具有可用容量的最近的資料中心。我們的**全球軟體負載平衡器 (Global Software Load Balancer, GSLB)** 在三個層級上執行負載平衡：

- **DNS 請求的地理負載平衡**（例如，對 www.google.com），在「前端負載平衡」中描述。
- **用戶服務層級的負載平衡**（例如，YouTube 或 Google 地圖）。
- **遠端程序呼叫 (Remote Procedure Call, RPC) 層級的負載平衡**，在「資料中心的負載平衡」中描述。

服務所有者為服務指定一個符號名稱、一個伺服器的 BNS 位址列表，以及每個位置的可用容量（通常以每秒查詢次數衡量）。然後，GSLB 將流量引導到這些 BNS 位址。

# 其他系統軟體

資料中心中的其他幾個組件也很重要。

## 鎖定服務 (Lock Service)

**Chubby** [Bur06] 鎖定服務提供了一個類似檔案系統的 API 來維護鎖。Chubby 處理跨資料中心位置的這些鎖。它使用 Paxos 協定來實現非同步共識（請參閱「管理關鍵狀態：為可靠性而生的分散式共識」）。

Chubby 在**主節點選舉 (master election)** 中也扮演著重要角色。當一個服務為了可靠性而運行五個工作副本，但只有一個副本可以執行實際工作時，Chubby 就被用來選擇哪個副本可以繼續。

必須保持一致的資料非常適合儲存在 Chubby 中。因此，BNS 使用 Chubby 來儲存 BNS 路徑和 IP 位址:埠號對之間的對應關係。

## 監控和警報

我們希望確保所有服務都按要求運行。因此，我們運行了許多我們的 **Borgmon** 監控程式的實例（請參閱「來自時間序列資料的實用警報」）。Borgmon 定期從被監控的伺服器上「抓取 (scrapes)」指標。這些指標可以立即用於警報，也可以儲存起來用於歷史概覽（例如，圖表）。我們可以用多種方式使用監控：

- 為緊急問題設定警報。
- 比較行為：軟體更新是否讓伺服器變得更快？
- 檢查資源消耗行為隨時間的演變，這對容量規劃至關重要。

# 我們的軟體基礎設施

我們的軟體架構旨在最有效地利用我們的硬體基礎設施。我們的程式碼是高度多執行緒的，因此一個任務可以輕鬆地使用多個核心。為了方便儀表板、監控和除錯，每個伺服器都有一個 HTTP 伺服器，為給定的任務提供診斷和統計資訊。

Google 的所有服務都使用一個名為 **Stubby** 的**遠端程序呼叫 (Remote Procedure Call, RPC)** 基礎設施進行通訊；一個開源版本 gRPC 是可用的。11 通常，即使需要執行本地程式中的子程序呼叫，也會進行 RPC 呼叫。這使得在需要更多模組化或伺服器程式碼庫增長時，更容易將呼叫重構到不同的伺服器中。GSLB 可以像負載平衡外部可見服務一樣負載平衡 RPC。

一個伺服器從其**前端 (frontend)** 接收 RPC 請求，並向其**後端 (backend)** 發送 RPC。用傳統術語來說，前端稱為客戶端，後端稱為伺服器。

資料通過**協定緩衝區 (protocol buffers)** 12（通常縮寫為 "protobufs"）在 RPC 之間傳輸，這類似於 Apache 的 Thrift。協定緩衝區在序列化結構化資料方面比 XML 有許多優點：它們更簡單易用、小 3 到 10 倍、快 20 到 100 倍，而且歧義更少。

# 我們的開發環境

開發速度對 Google 非常重要，因此我們建立了一個完整的開發環境來利用我們的基礎設施 [Mor12b]。

除了一些擁有自己開源儲存庫的團隊（例如，Android 和 Chrome），Google 軟體工程師都在一個單一的共享儲存庫中工作 [Pot16]。這對我們的工作流程有幾個重要的實際影響：

- 如果工程師在他們專案之外的組件中遇到問題，他們可以修復問題，將建議的變更（「變更列表 (changelist)」或 **CL**）發送給所有者進行審查，並將 CL 提交到主線。
- 對工程師自己專案中原始碼的變更需要審查。所有軟體在提交前都會經過審查。

當軟體建置時，建置請求會被發送到資料中心的建置伺服器。即使是大型建置也能快速執行，因為許多建置伺服器可以並行編譯。這個基礎設施也用於持續測試。每次提交 CL 時，測試都會在所有可能直接或間接依賴該 CL 的軟體上運行。如果框架確定該變更可能破壞了系統的其他部分，它會通知提交變更的所有者。一些專案使用「綠燈時推送 (push-on-green)」系統，即新版本在通過測試後會自動推送到生產環境。

# 莎士比亞：一個範例服務

為了提供一個服務在 Google 生產環境中如何部署的假設模型，讓我們來看一個與多種 Google 技術互動的範例服務。假設我們想提供一個服務，讓您可以確定莎士比亞所有作品中某個特定單詞的使用位置。

我們可以將這個系統分為兩部分：

- 一個批次處理組件，讀取莎士比亞的所有文本，創建一個索引，並將索引寫入一個 Bigtable。這個工作只需要運行一次，或者可能非常不頻繁（因為你永遠不知道是否會發現新的文本！）。
- 一個應用程式前端，處理終端用戶的請求。這個工作總是在運行，因為所有時區的用戶都想在莎士比亞的書中進行搜索。

批次處理組件是一個包含三個階段的 MapReduce。

1.  **映射 (mapping)** 階段讀取莎士比亞的文本並將其拆分為單個單詞。如果由多個工作執行緒並行執行，速度會更快。
2.  **洗牌 (shuffle)** 階段按單詞對元組進行排序。
3.  在 **歸約 (reduce)** 階段，創建一個 (單詞, 位置列表) 的元組。

每個元組都被寫入 Bigtable 的一行中，使用單詞作為鍵。

## 一個請求的生命週期

圖 2-4 顯示了一個用戶的請求是如何被服務的：首先，用戶將他們的瀏覽器指向 shakespeare.google.com。為了獲取相應的 IP 位址，用戶的設備通過其 DNS 伺服器解析該位址 (1)。這個請求最終會到達 Google 的 DNS 伺服器，該伺服器與 GSLB 通訊。由於 GSLB 跟踪跨地區前端伺服器之間的流量負載，它會選擇將哪個伺服器 IP 位址發送給該用戶。

瀏覽器連接到該 IP 上的 HTTP 伺服器。該伺服器（名為 Google 前端，或 GFE）是一個反向代理，它終止 TCP 連接 (2)。GFE 查詢需要哪個服務（網頁搜索、地圖，或在本例中是莎士比亞）。再次使用 GSLB，伺服器找到一個可用的莎士比亞前端伺服器，並向該伺服器發送一個包含 HTTP 請求的 RPC (3)。

莎士比亞伺服器分析 HTTP 請求並構造一個包含要查找的單詞的協定緩衝區。莎士比亞前端伺服器現在需要聯繫莎士比亞後端伺服器：前端伺服器聯繫 GSLB 以獲取一個合適且未過載的後端伺服器的 BNS 位址 (4)。該莎士比亞後端伺服器現在聯繫一個 Bigtable 伺服器以獲取請求的資料 (5)。

答案被寫入回覆協定緩衝區並返回給莎士比亞後端伺服器。後端將包含結果的協定緩衝區交給莎士比亞前端伺服器，後者組裝 HTML 並將答案返回給用戶。

這整個事件鏈在眨眼之間完成——僅僅幾百毫秒！由於涉及許多活動部件，因此存在許多潛在的故障點；特別是，一個發生故障的 GSLB 會造成嚴重破壞。然而，Google 嚴格的測試和謹慎的推出策略，以及我們主動的錯誤恢復方法，如優雅降級 (graceful degradation)，使我們能夠提供我們的用戶所期望的可靠服務。畢竟，人們經常使用 www.google.com 來檢查他們的網路連接是否設置正確。

## 工作和資料組織

負載測試確定我們的後端伺服器每秒可以處理約 100 次查詢 (QPS)。通過一組有限用戶進行的試驗，我們預計峰值負載約為 3,470 QPS，因此我們至少需要 35 個任務。然而，以下考慮因素意味著我們在工作中至少需要 37 個任務，即 **N+2**：

- 在更新期間，一次會有一個任務不可用，剩下 36 個任務。
- 在任務更新期間可能會發生機器故障，只剩下 35 個任務，剛好足以應對峰值負載。13

對用戶流量的更仔細檢查顯示，我們的高峰使用量分佈在全球：北美 1,430 QPS，南美 290 QPS，歐洲和非洲 1,400 QPS，亞洲和澳大利亞 350 QPS。我們沒有將所有後端都放在一個站點，而是將它們分佈在美國、南美、歐洲和亞洲。考慮到每個地區的 N+2 冗餘，我們最終在美國有 17 個任務，歐洲有 16 個任務，亞洲有 6 個任務。然而，我們決定在南美使用 4 個任務（而不是 5 個），將 N+2 的開銷降低到 N+1。在這種情況下，我們願意容忍輕微的更高延遲風險，以換取更低的硬體成本：如果當我們的南美資料中心超載時，GSLB 將流量從一個大洲重定向到另一個大洲，我們可以節省 20% 的硬體資源。在較大的地區，我們會將任務分散到兩個或三個叢集中以增加彈性。

因為後端需要聯繫持有資料的 Bigtable，我們也需要策略性地設計這個儲存元件。亞洲的後端聯繫美國的 Bigtable 會增加大量的延遲，所以我們在每個地區都複製了 Bigtable。Bigtable 複製在兩方面對我們有幫助：它在 Bigtable 伺服器發生故障時提供了彈性，並且降低了資料存取延遲。雖然 Bigtable 只提供最終一致性，但這不是一個主要問題，因為我們不需要經常更新內容。

我們在這裡介紹了很多術語；雖然您不需要全部記住，但它對於理解我們稍後將提到的許多其他系統很有用。

9 嗯，大致相同。基本上。除了那些不同的東西。一些資料中心最終會有多代計算硬體，有時我們會在資料中心建成後對其進行擴充。但在很大程度上，我們的資料中心硬體是同質的。

10 一些讀者可能更熟悉 Borg 的後代 Kubernetes——一個由 Google 於 2014 年啟動的開源容器叢集編排框架；請參閱 https://kubernetes.io 和 [Bur16]。有關 Borg 和 Apache Mesos 之間相似之處的更多詳細資訊，請參閱 [Ver15]。

11 請參閱 https://grpc.io。

12 協定緩衝區是一種語言中立、平台中立的可擴展機制，用於序列化結構化資料。有關更多詳細資訊，請參閱 https://developers.google.com/protocol-buffers/。

13 我們假設在我們的環境中，兩個任務同時失敗的機率低到可以忽略不計。在其他環境中，諸如機櫃頂端交換器或配電等單點故障可能會使這個假設無效。